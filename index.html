<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Code Analysis</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        h1 { color: #2c3e50; text-align: center; }
        h3 { color: #34495e; margin-top: 20px; }
        pre { background-color: #1e1e1e; color: #dcdcdc; padding: 15px; border-radius: 5px; overflow-x: auto; }
        code { font-family: 'Courier New', Courier, monospace; font-size: 14px; }
        code.markdown { background-color: #2e2e2e; color: #e0e0e0; }
        ul { list-style-type: none; padding: 0; }
        li { margin-bottom: 20px; }
    </style>
</head>
<body>
    <h1>Python Code Analysis</h1>
    <h2>Python and JSON Files</h2>
    <ul>
        <li><h3>File: .\ai_models.py</h3>
        <pre><code>import torch
from torch import nn
import pandas as pd
from sklearn.model_selection import train_test_split
from torch.optim import Adam
import onnx
import onnxruntime as ort
from onnxmltools import convert_xgboost, convert_sklearn, convert_lightgbm
from onnxconverter_common import FloatTensorType, convert_float_to_float16
from transformers import pipeline, TimeSeriesTransformerModel, TimeSeriesTransformerConfig, DistilBertTokenizer, DistilBertForSequenceClassification
import logging
from scipy.stats import ks_2samp
import os
import numpy as np
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
import lightgbm as lgb
from pathlib import Path
import joblib
import redis
from sklearn.metrics import mean_squared_error, r2_score
from utils import save_data, check_hardware, get_redis_client
from datetime import timedelta, datetime
import asyncio
import aiohttp
from textblob import TextBlob
import aiosqlite

# 全局特徵常量，避免重複定義
FEATURES = ['close', 'RSI', 'MACD', 'Stoch_k', 'ADX', 'BB_upper', 'BB_lower', 'EMA_12', 'EMA_26', 'fed_funds_rate']

class LSTMModel(nn.Module):
    """LSTM 模型：用於價格預測，捕捉時間序列模式。"""
    def __init__(self, input_size=10, hidden_size=50, num_layers=2, output_size=1, device=torch.device('cpu')):
        """初始化 LSTM 模型，支援多個技術指標和經濟數據的輸入大小。"""
        super().__init__()
        self.device = device
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
        self.to(device)
 
    def forward(self, x):
        # 關鍵邏輯：將輸入數據移動到指定設備，並通過 LSTM 和全連接層進行前向傳播。
        x = x.to(self.device)
        _, (h_n, _) = self.lstm(x)
        return self.fc(h_n[-1])
def validate_data(df: pd.DataFrame, required_features: list) -&gt; bool:
    """驗證輸入數據是否包含必要的特徵欄位。"""
    missing_features = [f for f in required_features if f not in df.columns]
    if missing_features:
        logging.error(f"缺少必要特徵: {missing_features}")
        return False
    if df[required_features].isna().any().any():
        logging.error("數據包含 NaN 值")
        return False
    return True
def train_lstm_model(df: pd.DataFrame, epochs: int = 50, device=torch.device('cpu')):
    """訓練 LSTM：使用歷史數據訓練，保存為 ONNX 格式。"""
    # 函數說明：此函數負責訓練 LSTM 模型，使用指定的特徵進行價格預測，並將模型轉換為 ONNX 格式以便跨平台使用。
    try:
        if not validate_data(df, FEATURES):
            return None
        # 關鍵邏輯：移除 NaN 值並準備輸入 X 和目標 y（下一期的 Close 值）。
        X = df[FEATURES].dropna().values[:-1]
        y = df['close'].shift(-1).dropna().values
        if len(X) != len(y):
            logging.error("X 和 y 長度不匹配")
            return None
        if len(X) == 0:
            logging.error("X 數據為空，無法訓練 LSTM")
            return None
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        model = LSTMModel(input_size=len(FEATURES), device=device)
        optimizer = Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        # 關鍵邏輯：訓練循環，使用反向傳播更新模型參數。
        for epoch in range(epochs):
            optimizer.zero_grad()
            output = model(torch.tensor(X_train, dtype=torch.float32, device=device).unsqueeze(1))
            loss = criterion(output.squeeze(), torch.tensor(y_train, dtype=torch.float32, device=device))
            loss.backward()
            optimizer.step()
     
        # 保存 PyTorch 模型
        torch.save(model.state_dict(), 'models/lstm_model.pth')
        # 轉換為 ONNX
        dummy_input = torch.tensor(X_train[:1], dtype=torch.float32).unsqueeze(1)
        torch.onnx.export(model, dummy_input, "models/lstm_model.onnx", input_names=["input"], output_names=["output"], opset_version=11)
        # 量化為 FP16
        model_fp32 = onnx.load("models/lstm_model.onnx")
        model_fp16 = convert_float_to_float16(model_fp32)
        onnx.save(model_fp16, "models/lstm_model_quantized.onnx")
        logging.info("LSTM 模型訓練完成並轉換為 ONNX")
        return model
    except Exception as e:
        logging.error(f"LSTM 訓練錯誤: {e}")
        return None
def train_xgboost_model(df: pd.DataFrame):
    """訓練 XGBoost 模型：用於短期價格預測，保存為 ONNX 格式。"""
    # 函數說明：訓練 XGBoost 模型，用於短期價格預測，並計算性能指標後轉換為 ONNX 格式。
    try:
        if not validate_data(df, FEATURES):
            return None
        X = df[FEATURES].dropna().values[:-1]
        y = df['close'].shift(-1).dropna().values
        if len(X) != len(y):
            logging.error("X 和 y 長度不匹配")
            return None
        if len(X) == 0:
            logging.error("X 數據為空，無法訓練 XGBoost")
            return None
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1)
        model.fit(X_train, y_train)
        # 關鍵邏輯：計算模型性能指標，如 RMSE 和 R² 分數。
        y_pred = model.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        r2 = r2_score(y_test, y_pred)
        logging.info(f"XGBoost 價格預測性能：RMSE={rmse:.4f}, R²={r2:.4f}")
        joblib.dump(model, 'models/xgboost_model.pkl')
        # 轉換為 ONNX
        onnx_model = convert_xgboost(model, initial_types=[('input', FloatTensorType([None, len(FEATURES)]))])
        onnx.save(onnx_model, "models/xgboost_model.onnx")
        # 量化為 FP16
        model_fp32 = onnx.load("models/xgboost_model.onnx")
        model_fp16 = convert_float_to_float16(model_fp32)
        onnx.save(model_fp16, "models/xgboost_model_quantized.onnx")
        logging.info("XGBoost 模型訓練完成並轉換為 ONNX")
        return model
    except Exception as e:
        logging.error(f"XGBoost 訓練錯誤: {e}")
        return None
def train_random_forest_model(df: pd.DataFrame):
    """訓練隨機森林模型：用於短期價格預測，保存為 ONNX 格式。"""
    # 函數說明：訓練 RandomForest 模型，用於中期價格預測，並轉換為 ONNX 格式。
    try:
        if not validate_data(df, FEATURES):
            return None
        X = df[FEATURES].dropna().values[:-1]
        y = df['close'].shift(-1).dropna().values
        if len(X) != len(y):
            logging.error("X 和 y 長度不匹配")
            return None
        if len(X) == 0:
            logging.error("X 數據為空，無法訓練 RandomForest")
            return None
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        joblib.dump(model, 'models/rf_model.pkl')
        # 轉換為 ONNX
        onnx_model = convert_sklearn(model, initial_types=[('input', FloatTensorType([None, len(FEATURES)]))])
        onnx.save(onnx_model, "models/rf_model.onnx")
        # 量化為 FP16
        model_fp32 = onnx.load("models/rf_model.onnx")
        model_fp16 = convert_float_to_float16(model_fp32)
        onnx.save(model_fp16, "models/rf_model_quantized.onnx")
        logging.info("RandomForest 模型訓練完成並轉換為 ONNX")
        return model
    except Exception as e:
        logging.error(f"RandomForest 訓練錯誤: {e}")
        return None
def train_lightgbm_model(df: pd.DataFrame):
    """訓練 LightGBM 模型：用於波動性預測（ATR），保存為 ONNX 格式。"""
    # 函數說明：訓練 LightGBM 模型，用於波動性預測，並與 XGBoost 比較性能後轉換為 ONNX 格式。
    try:
        if not validate_data(df, FEATURES):
            return None
        X = df[FEATURES].dropna().values[:-1]
        y = df['ATR'].shift(-1).dropna().values
        if len(X) != len(y):
            logging.error("X 和 y 長度不匹配")
            return None
        if len(X) == 0:
            logging.error("X 數據為空，無法訓練 LightGBM")
            return None
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1)
        model.fit(X_train, y_train)
        # 關鍵邏輯：計算 LightGBM 性能指標，並訓練 XGBoost 進行比較。
        y_pred = model.predict(X_test)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        r2 = r2_score(y_test, y_pred)
        logging.info(f"LightGBM 波動性預測性能：RMSE={rmse:.4f}, R²={r2:.4f}")
        # 比較 XGBoost 的波動性預測性能
        xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1)
        xgb_model.fit(X_train, y_train)
        xgb_pred = xgb_model.predict(X_test)
        xgb_rmse = mean_squared_error(y_test, xgb_pred, squared=False)
        xgb_r2 = r2_score(y_test, xgb_pred)
        logging.info(f"XGBoost 波動性預測性能：RMSE={xgb_rmse:.4f}, R²={xgb_r2:.4f}")
        logging.info(f"性能比較：LightGBM RMSE={rmse:.4f} vs XGBoost RMSE={xgb_rmse:.4f}, LightGBM R²={r2:.4f} vs XGBoost R²={xgb_r2:.4f}")
        joblib.dump(model, 'models/lightgbm_model.pkl')
        # 轉換為 ONNX
        onnx_model = convert_lightgbm(model, initial_types=[('input', FloatTensorType([None, len(FEATURES)]))])
        onnx.save(onnx_model, "models/lightgbm_model.onnx")
        # 量化為 FP16
        model_fp32 = onnx.load("models/lightgbm_model.onnx")
        model_fp16 = convert_float_to_float16(model_fp32)
        onnx.save(model_fp16, "models/lightgbm_model_quantized.onnx")
        logging.info("LightGBM 模型訓練完成並轉換為 ONNX")
        return model
    except Exception as e:
        logging.error(f"LightGBM 訓練錯誤: {e}")
        return None
def train_timeseries_transformer(df: pd.DataFrame, epochs: int = 10, device=torch.device('cpu')):
    """訓練 TimeSeriesTransformer 模型：用於時間序列預測，保存為 ONNX 格式。"""
    # 函數說明：訓練 TimeSeriesTransformer 模型，用於時間序列預測，處理序列數據並轉換為 ONNX 格式。
    try:
        seq_len = 60
        # 關鍵邏輯：移除 NaN 值並構造時間序列輸入 X 和目標 y。
        df_clean = df.dropna(subset=FEATURES + ['close'])
        num_seq = len(df_clean) - seq_len
        if num_seq &lt;= 0:
            logging.error("數據不足以構造時間序列，無法訓練 TimeSeriesTransformer")
            return None
        X = np.array([df_clean[FEATURES].iloc[i:i+seq_len].values for i in range(num_seq)])
        y = df_clean['close'].iloc[seq_len:].values
        if len(X) != len(y):
            logging.error("X 和 y 長度不匹配")
            return None
        config = TimeSeriesTransformerConfig(
            input_size=len(FEATURES), time_series_length=seq_len, prediction_length=1, d_model=64
        )
        model = TimeSeriesTransformerModel(config).to(device)
        optimizer = Adam(model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        # 關鍵邏輯：訓練循環，使用反向傳播更新模型參數。
        for epoch in range(epochs):
            optimizer.zero_grad()
            output = model(torch.tensor(X_train, dtype=torch.float32, device=device)).logits
            loss = criterion(output.squeeze(), torch.tensor(y_train, dtype=torch.float32, device=device))
            loss.backward()
            optimizer.step()
        # 保存 PyTorch 模型
        torch.save(model.state_dict(), 'models/timeseries_transformer.pth')
        # 轉換為 ONNX
        dummy_input = torch.tensor(X_train[:1], dtype=torch.float32, device=device)
        torch.onnx.export(model, dummy_input, "models/timeseries_transformer.onnx", input_names=["input"], output_names=["output"], opset_version=11)
        # 量化為 FP16
        model_fp32 = onnx.load("models/timeseries_transformer.onnx")
        model_fp16 = convert_float_to_float16(model_fp32)
        onnx.save(model_fp16, "models/timeseries_transformer_quantized.onnx")
        logging.info("TimeSeriesTransformer 模型訓練完成並轉換為 ONNX")
        return model
    except Exception as e:
        logging.error(f"TimeSeriesTransformer 訓練錯誤: {e}")
        return None
def train_distilbert(df: pd.DataFrame, device=torch.device('cpu')):
    """訓練 DistilBERT 模型：用於情緒分析，保存為 ONNX 格式。"""
    # 函數說明：訓練 DistilBERT 模型，用於情緒分析，處理文本數據並轉換為 ONNX 格式。
    try:
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased').to(device)
        texts = df['tweets'].dropna().tolist()[:100]
        labels = df['sentiment'].dropna().tolist()[:100]
        if not texts or len(texts) != len(labels):
            logging.error("無效的文本或標籤數據")
            return None, None
        # 關鍵邏輯：對文本進行 tokenization 並準備輸入。
        inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
        labels = torch.tensor(labels, dtype=torch.long, device=device)
        optimizer = Adam(model.parameters(), lr=5e-5)
        for epoch in range(3):
            optimizer.zero_grad()
            outputs = model(**inputs).logits
            loss = nn.CrossEntropyLoss()(outputs, labels)
            loss.backward()
            optimizer.step()
        # 保存 PyTorch 模型
        torch.save(model.state_dict(), 'models/distilbert_model.pth')
        # 轉換為 ONNX
        dummy_input = {
            'input_ids': tokenizer(['dummy text'], return_tensors="pt", padding=True, truncation=True)['input_ids'].to(device),
            'attention_mask': tokenizer(['dummy text'], return_tensors="pt", padding=True, truncation=True)['attention_mask'].to(device)
        }
        torch.onnx.export(model, (dummy_input['input_ids'], dummy_input['attention_mask']),
                          "models/distilbert_model.onnx", input_names=["input_ids", "attention_mask"], output_names=["output"], opset_version=11)
        # 量化為 FP16
        model_fp32 = onnx.load("models/distilbert_model.onnx")
        model_fp16 = convert_float_to_float16(model_fp32)
        onnx.save(model_fp16, "models/distilbert_model_quantized.onnx")
        logging.info("DistilBERT 模型訓練完成並轉換為 ONNX")
        return model, tokenizer
    except Exception as e:
        logging.error(f"DistilBERT 訓練錯誤: {e}")
        return None, None
async def predict_sentiment(date: str, db_path: str, config: dict) -&gt; float:
    """情緒分析：從 X API 獲取推文，計算 polarity 並儲存結果，使用 DistilBERT，加入 Redis 快取。"""
    # 函數說明：進行情緒分析，從 X (Twitter) API 獲取推文，使用 DistilBERT 和 TextBlob 計算情緒分數，並快取結果。
    use_redis = config.get('system_config', {}).get('use_redis', True)
   
    redis_client = get_redis_client(config)
    try:
        end_date = pd.to_datetime(date)
        start_date = end_date - timedelta(days=1)
        cache_key = f"sentiment_{end_date.strftime('%Y-%m-%d')}"
        # 關鍵邏輯：檢查 Redis 快取，若存在則直接返回。
        # 檢查 Redis 快取
        if use_redis and redis_client:
            try:
                cached = redis_client.get(cache_key)
                if cached:
                    logging.info(f"從 Redis 快取載入情緒分數: {cache_key}")
                    return float(cached)
            except redis.RedisError as e:
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Redis 快取查詢失敗：{str(e)}，跳過 Redis")
                logging.warning(f"Redis cache query failed: {str(e)}，falling back to API", extra={'mode': 'predict_sentiment'})
        start_time = start_date.strftime('%Y-%m-%dT00:00:00Z')
        end_time = end_date.strftime('%Y-%m-%dT23:59:59Z')
        X_BEARER_TOKEN = config['api_key'].get('x_bearer_token', '')
        if not X_BEARER_TOKEN:
            logging.error("X Bearer Token 未配置")
            return 0.0
        query = "(USDJPY OR USD/JPY OR 'Federal Reserve') lang:en"
        logging.info(f"從 X API 獲取推文：query={query}, start={start_time}, end={end_time}")
        url = "https://api.x.com/2/tweets/search/recent"
        headers = {"Authorization": f"Bearer {X_BEARER_TOKEN}"}
        params = {'query': query, 'start_time': start_time, 'end_time': end_time, 'max_results': 100}
        polarities = []
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')
        model.load_state_dict(torch.load('models/distilbert_model.pth'))
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.eval()
        async with aiohttp.ClientSession() as session:
            for attempt in range(5):
                try:
                    async with session.get(url, headers=headers, params=params, timeout=10) as response:
                        data = await response.json()
                        if response.status != 200 or 'data' not in data or not data['data']:
                            logging.warning(f"X API 數據為空或格式錯誤: {data}")
                            return 0.0
                        tweets = data['data']
                        # 關鍵邏輯：對每條推文計算情緒分數，結合 DistilBERT 和 TextBlob。
                        for tweet in tweets:
                            text = tweet['text']
                            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128).to(device)
                            with torch.no_grad():
                                outputs = model(**inputs).logits
                            score = torch.softmax(outputs, dim=1)[0][1].item() - torch.softmax(outputs, dim=1)[0][0].item()
                            tb_polarity = TextBlob(text).sentiment.polarity
                            combined_score = 0.7 * score + 0.3 * tb_polarity
                            polarities.append(combined_score)
                        break
                except Exception as e:
                    if attempt == 4:
                        logging.error(f"X API 獲取失敗: {str(e)}")
                        return 0.0
                    await asyncio.sleep(2 ** attempt * 4)
        if polarities:
            avg_score = sum(polarities) / len(polarities)
            logging.info(f"計算平均 polarity 分數: {avg_score} (DistilBERT 70%, TextBlob 30%)")
            # 存入 Redis 快取，設置 24 小時過期
            if use_redis and redis_client:
                try:
                    redis_client.setex(cache_key, 24 * 3600, str(avg_score))
                    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已將情緒分數快取至 Redis")
                    logging.info(f"Cached sentiment score to Redis: key={cache_key}")
                except redis.RedisError as e:
                    logging.warning(f"無法快取至 Redis: {str(e)}")
        else:
            avg_score = 0.0
            logging.warning("無推文數據，使用預設值 0.0")
        sentiment_df = pd.DataFrame({'date': [end_date], 'sentiment': [avg_score]})
        await save_data(sentiment_df, timeframe='1 day', db_path=db_path, data_type='sentiment')
        return avg_score
    except Exception as e:
        logging.error(f"情緒分析錯誤: {e}")
        return 0.0
def integrate_sentiment(polarity: float) -&gt; float:
    """整合情緒分數：將 polarity 轉換為決策調整值，並檢查極端情緒。"""
    # 函數說明：根據情緒分數調整決策值，若極端則暫停交易。
    if abs(polarity) &gt; 0.8:
        logging.warning(f"極端情緒分數: {polarity}，建議暫停交易")
        return 0.0
    if polarity &gt; 0.1:
        return 0.1
    elif polarity &lt; -0.1:
        return -0.1
    return 0.0
def detect_drift(old_data: pd.DataFrame, new_data: pd.DataFrame, threshold: float = 0.05) -&gt; bool:
    """檢測數據漂移：使用 KS 檢驗比較分佈。"""
    # 函數說明：使用 Kolmogorov-Smirnov 檢驗檢測數據分佈漂移。
    stat, p_value = ks_2samp(old_data['close'], new_data['close'])
    return p_value &lt; threshold
def predict(model_path: str, input_data: pd.DataFrame, provider='VitisAIExecutionProvider'):
    """使用 ONNX 模型進行推理，支援 NPU。"""
    # 函數說明：載入 ONNX 模型並進行預測，支持多種執行提供者。
    try:
        session = ort.InferenceSession(model_path, providers=[provider, 'CUDAExecutionProvider', 'CPUExecutionProvider'])
        X = input_data[FEATURES].iloc[-1:].values.astype(np.float32)
        return session.run(None, {'input': X})[0][0]
    except Exception as e:
        logging.error(f"ONNX 推理錯誤: {e}")
        return 0.0
def update_model(df: pd.DataFrame, model_path: str = 'models', session: str = 'normal', device_config: dict = None) -&gt; dict:
    """更新多模型：個別檢查並訓練或載入模型，根據時段加權預測。"""
    # 函數說明：更新多個模型，若數據漂移則重新訓練，並定義加權預測函數。
    try:
        model_dir = Path(model_path)
        model_dir.mkdir(exist_ok=True)
        models = {}
        # 關鍵邏輯：檢測數據漂移，若存在則重新訓練模型。
        old_data = df.iloc[:-1000] if len(df) &gt; 1000 else df
        new_data = df.iloc[-1000:]
        data_drift = detect_drift(old_data, new_data)
        device = device_config.get('lstm', torch.device('cpu')) if device_config else torch.device('cpu')
        df_clean = df[FEATURES].dropna()
        X = df_clean.values[:-1]
        y = df['close'].shift(-1).dropna().values
        # LSTM 模型
        lstm_path = model_dir / 'lstm_model.pth'
        lstm_onnx_path = model_dir / 'lstm_model_quantized.onnx'
        if not lstm_path.exists() or data_drift:
            models['lstm'] = train_lstm_model(df, device=device)
        else:
            models['lstm'] = lstm_onnx_path
            logging.info("載入現有 LSTM ONNX 模型")
        # XGBoost 模型
        xgboost_path = model_dir / 'xgboost_model.pkl'
        xgboost_onnx_path = model_dir / 'xgboost_model_quantized.onnx'
        if not xgboost_path.exists() or data_drift:
            models['xgboost'] = train_xgboost_model(df)
        else:
            models['xgboost'] = xgboost_onnx_path
            logging.info("載入現有 XGBoost ONNX 模型")
        # RandomForest 模型
        rf_path = model_dir / 'rf_model.pkl'
        rf_onnx_path = model_dir / 'rf_model_quantized.onnx'
        if not rf_path.exists() or data_drift:
            models['rf_model'] = train_random_forest_model(df)
        else:
            models['rf_model'] = rf_onnx_path
            logging.info("載入現有 RandomForest ONNX 模型")
        # LightGBM 模型
        lightgbm_path = model_dir / 'lightgbm_model.pkl'
        lightgbm_onnx_path = model_dir / 'lightgbm_model_quantized.onnx'
        if not lightgbm_path.exists() or data_drift:
            models['lightgbm'] = train_lightgbm_model(df)
        else:
            models['lightgbm'] = lightgbm_onnx_path
            logging.info("載入現有 LightGBM ONNX 模型")
        # TimeSeriesTransformer 模型
        transformer_path = model_dir / 'timeseries_transformer.pth'
        transformer_onnx_path = model_dir / 'timeseries_transformer_quantized.onnx'
        if not transformer_path.exists() or data_drift:
            models['timeseries_transformer'] = train_timeseries_transformer(df, device=device)
        else:
            models['timeseries_transformer'] = transformer_onnx_path
            logging.info("載入現有 TimeSeriesTransformer ONNX 模型")
        # DistilBERT 模型
        distilbert_path = model_dir / 'distilbert_model.pth'
        distilbert_onnx_path = model_dir / 'distilbert_model_quantized.onnx'
        if not distilbert_path.exists() or data_drift:
            models['distilbert'], _ = train_distilbert(df, device=device)
        else:
            models['distilbert'] = distilbert_onnx_path
            logging.info("載入現有 DistilBERT ONNX 模型")
        # 加權預測
        weights = {
            'lstm': 0.2, 'xgboost': 0.3, 'rf_model': 0.2, 'lightgbm': 0.2, 'timeseries_transformer': 0.1
        } if session == 'high_volatility' else {
            'lstm': 0.3, 'xgboost': 0.2, 'rf_model': 0.2, 'lightgbm': 0.2, 'timeseries_transformer': 0.1
        }
        def predict_price(input_data: pd.DataFrame):
            # 內部函數說明：根據輸入數據進行加權預測，忽略情緒模型。
            if input_data.empty or FEATURES[0] not in input_data.columns:
                logging.error("輸入數據為空或缺少必要欄位")
                return 0.0
            predictions = {}
            for name, model in models.items():
                if name == 'distilbert':
                    continue
                if isinstance(model, str):
                    predictions[name] = predict(model, input_data)
                else:
                    X = input_data[FEATURES].iloc[-1:].values
                    if name == 'lstm' or name == 'timeseries_transformer':
                        X_tensor = torch.tensor(X, dtype=torch.float32, device=device).unsqueeze(1)
                        model.eval()
                        with torch.no_grad():
                            predictions[name] = model(X_tensor).item()
                    else:
                        predictions[name] = model.predict(X)[0]
            final_pred = sum(weights[name] * pred for name, pred in predictions.items())
            return final_pred
        models['predict'] = predict_price
        logging.info("多模型更新完成")
        return models
    except Exception as e:
        logging.error(f"多模型更新錯誤: {e}")
        return {}</code></pre></li>

        <li><h3>File: .\code_to_html.py</h3>
        <pre><code>import os

def py_to_html(root_dir: str = '.', output_file: str = 'index.html'):
    """將所有 PY、JSON 和 MD 檔內容合併到 HTML，Markdown 檔案放在最後。
    邏輯：遍歷目錄，讀取 PY、JSON 和 MD 檔，寫入 HTML，使用 &lt;ul&gt; 和 &lt;h3&gt; 組織檔案。
    """
    html_content = """&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;Python Code Analysis&lt;/title&gt;
    &lt;style&gt;
        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }
        h1 { color: #2c3e50; text-align: center; }
        h3 { color: #34495e; margin-top: 20px; }
        pre { background-color: #1e1e1e; color: #dcdcdc; padding: 15px; border-radius: 5px; overflow-x: auto; }
        code { font-family: 'Courier New', Courier, monospace; font-size: 14px; }
        code.markdown { background-color: #2e2e2e; color: #e0e0e0; }
        ul { list-style-type: none; padding: 0; }
        li { margin-bottom: 20px; }
    &lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;Python Code Analysis&lt;/h1&gt;
    &lt;h2&gt;Python and JSON Files&lt;/h2&gt;
    &lt;ul&gt;
"""

    # 儲存 Python/JSON 和 Markdown 內容分開處理
    py_json_content = ""
    md_content = ""

    with open(output_file, 'w', encoding='utf-8') as out:
        exclude = ['venv', 'test']
        for subdir, dirs, files in os.walk(root_dir, topdown=True):
            for jump in exclude:
                if subdir == root_dir and jump in dirs:
                    dirs.remove(jump)
                    print(f"跳過根目錄的資料夾：{jump}")
            for file in files:
                if file.endswith(('.py', '.json')) and not file.endswith(('py.py', 'txt.py')) and not file.startswith(('grafana_dashboard', 'api_key', 'data_fetcher', 'model')):
                    path = os.path.join(subdir, file)
                    print(f"合併 Python/JSON 檔案：{path}")
                    py_json_content += f'        &lt;li&gt;&lt;h3&gt;File: {path}&lt;/h3&gt;\n'
                    py_json_content += '        &lt;pre&gt;&lt;code&gt;'
                    with open(path, 'r', encoding='utf-8') as f:
                        # Escape HTML special characters to ensure valid rendering
                        content = f.read().replace('&amp;', '&amp;amp;').replace('&lt;', '&amp;lt;').replace('&gt;', '&amp;gt;')
                        py_json_content += content
                    py_json_content += '&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n\n'
                elif file.endswith('.md'):
                    path = os.path.join(subdir, file)
                    print(f"合併 Markdown 檔案：{path}")
                    md_content += f'        &lt;li&gt;&lt;h3&gt;Markdown File: {path}&lt;/h3&gt;\n'
                    md_content += '        &lt;pre&gt;&lt;code class="markdown"&gt;'
                    with open(path, 'r', encoding='utf-8') as f:
                        # Escape HTML special characters for Markdown
                        content = f.read().replace('&amp;', '&amp;amp;').replace('&lt;', '&amp;lt;').replace('&gt;', '&amp;gt;')
                        md_content += content
                    md_content += '&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;\n\n'

        # 組合 HTML：先 Python/JSON，後 Markdown
        html_content += py_json_content
        html_content += """    &lt;/ul&gt;
    &lt;h2&gt;Markdown Files&lt;/h2&gt;
    &lt;ul&gt;
"""
        html_content += md_content
        html_content += """    &lt;/ul&gt;
&lt;/body&gt;
&lt;/html&gt;"""
        out.write(html_content)
    
    print(f"合併完成：{output_file}")

if __name__ == "__main__":
    py_to_html()</code></pre></li>

        <li><h3>File: .\data_acquisition.py</h3>
        <pre><code>import yfinance as yf
import pandas as pd
import pandas_ta as ta
import json
import time
import aiohttp
import aiosqlite
import investpy
from utils import initialize_db, save_data, get_proxy, test_proxy, fetch_api_data, get_redis_client, filter_future_dates
from datetime import datetime, timedelta
import asyncio
import os
from pathlib import Path
import logging
import redis
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import traceback
# 速率限制器類
class RateLimiter:
    def __init__(self, calls: int, period: float):
        """初始化速率限制器：calls 次/period 秒"""
        self.calls = calls
        self.period = period
        self.timestamps = []

    async def wait(self):
        """等待直到允許下一次請求"""
        current_time = time.time()
        # 移除超出時間窗口的時間戳
        self.timestamps = [t for t in self.timestamps if current_time - t &lt; self.period]
        if len(self.timestamps) &gt;= self.calls:
            # 等待直到最早的時間戳超出窗口
            sleep_time = self.period - (current_time - self.timestamps[0])
            if sleep_time &gt; 0:
                logging.info(f"Rate limiter: Waiting {sleep_time:.2f} seconds for next request")
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Rate limiter: Waiting {sleep_time:.2f} seconds")
                await asyncio.sleep(sleep_time)
        self.timestamps.append(time.time())

# 全局 Polygon API 速率限制器：每 60 秒 5 次
polygon_rate_limiter = RateLimiter(calls=5, period=60.0)

async def fetch_sentiment_data(date: str, db_path: str, config: dict) -&gt; pd.DataFrame:
    """從 X API 獲取推文並儲存到 SQLite 的 tweets 表。"""
    try:
        end_date = pd.to_datetime(date)
        start_date = end_date - timedelta(days=1)
        cache_key = f"tweets_{end_date.strftime('%Y-%m-%d')}"
        redis_client = get_redis_client(config)
        
        # 檢查 Redis 快取
        if redis_client:
            try:
                cached = redis_client.get(cache_key)
                if cached:
                    logging.info(f"從 Redis 快取載入推文數據: {cache_key}")
                    return pd.read_json(cached)
            except redis.RedisError as e:
                logging.warning(f"Redis 快取查詢失敗: {str(e)}")
        
        # X API 請求
        start_time = start_date.strftime('%Y-%m-%dT00:00:00Z')
        end_time = end_date.strftime('%Y-%m-%dT23:59:59Z')
        X_BEARER_TOKEN = config['api_key'].get('x_bearer_token', '')
        if not X_BEARER_TOKEN:
            logging.error("X Bearer Token 未配置")
            return pd.DataFrame()
        
        url = "https://api.x.com/2/tweets/search/recent"
        headers = {"Authorization": f"Bearer {X_BEARER_TOKEN}"}
        params = {'query': "(USDJPY OR USD/JPY OR 'Federal Reserve') lang:en", 'start_time': start_time, 'end_time': end_time, 'max_results': 100}
        data = await fetch_api_data(url, headers=headers, params=params)
        
        if 'data' not in data or not data['data']:
            logging.warning(f"X API 數據為空或格式錯誤: {data}")
            return pd.DataFrame()
        
        # 構建推文數據
        tweets = [
            {'date': end_date, 'tweet_id': tweet['id'], 'text': tweet['text']}
            for tweet in data['data']
        ]
        tweets_df = pd.DataFrame(tweets)
        # 儲存結果
        if not tweets_df.empty:
            if redis_client:
                try:
                    redis_client.setex(cache_key, 24 * 3600, tweets_df.to_json())
                    logging.info(f"已快取推文數據至 Redis: {cache_key}")
                except redis.RedisError as e:
                    logging.warning(f"無法快取至 Redis: {str(e)}")
            await save_data(tweets_df, timeframe='1 day', db_path=db_path, data_type='tweets')
            logging.info(f"儲存 {len(tweets_df)} 條推文到 SQLite")
        else:
            logging.warning("無推文數據")
        
        return tweets_df
    except Exception as e:
        logging.error(f"推文數據獲取錯誤: {str(e)}, traceback={traceback.format_exc()}")
        return pd.DataFrame()

async def fetch_data(primary_api: str = 'polygon', backup_apis: list = ['yfinance', 'fcs', 'fixer'], date_range: dict = None, timeframe: str = '1d', db_path: str = "C:\\Trading\\data\\trading_data.db", config: dict = None) -&gt; pd.DataFrame:
    """獲取資料：支援多 API，優先使用 primary，失敗則備用。使用 Redis 和 SQLite 快取減少呼叫，加入缺失值填補。"""
    def normalize_timeframe(tf: str) -&gt; str:
        mapping = {'1 hour': '1h', '4 hours': '4h', '1 day': '1d'}
        return mapping.get(tf, tf)
    timeframe = normalize_timeframe(timeframe)
    use_redis = config.get('system_config', {}).get('use_redis', True)
    
    redis_client = get_redis_client(config)
    key = f'usd_jpy_data_{timeframe}'
    start_date = pd.to_datetime(date_range['start'] if date_range else "2025-01-01")
    end_date = pd.to_datetime(date_range['end'] if date_range else "2025-08-25")
    CSV_PATH = Path(config['system_config']['root_dir']) / 'data' / f'usd_jpy_{timeframe}.csv'
    # 定義時間框架映射和各 API 的參數
    interval_map = {
        '1min': {'multiplier': 1, 'timespan': 'minute', 'yfinance': '1m', 'fcs': '1min', 'fixer': None},
        '5min': {'multiplier': 5, 'timespan': 'minute', 'yfinance': '5m', 'fcs': '5min', 'fixer': None},
        '1h': {'multiplier': 1, 'timespan': 'hour', 'yfinance': '1h', 'fcs': '1hour', 'fixer': None},
        '4h': {'multiplier': 4, 'timespan': 'hour', 'yfinance': '4h', 'fcs': '4hour', 'fixer': None},
        '1d': {'multiplier': 1, 'timespan': 'day', 'yfinance': '1d', 'fcs': '1day', 'fixer': '1d'}
    }
    if timeframe not in interval_map:
        logging.error(f"無效的時間框架: {timeframe}")
        return pd.DataFrame()
    interval = interval_map[timeframe]
    # 第一層快取：Redis
    if use_redis and redis_client:
        try:
            cached = redis_client.get(key)
            if cached:
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 從 Redis 快取載入資料")
                logging.info(f"Loaded data from Redis cache: key={key}, timeframe={timeframe}")
                df = pd.read_json(cached)
                df = fill_missing_values(df)
                return df[['date', 'open', 'high', 'low', 'close', 'volume', 'n', 'vw']]
        except redis.RedisError as e:
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Redis 快取查詢失敗：{str(e)}")
            logging.warning(f"Redis cache query failed: {str(e)}, timeframe={timeframe}, falling back to SQLite", extra={'mode': 'fetch_data'})
    # 第二層快取：SQLite
    await initialize_db(db_path)
    try:
        async with aiosqlite.connect(db_path, timeout=10) as conn:
            cursor = await conn.execute("SELECT * FROM ohlc WHERE timeframe = ? AND date BETWEEN ? AND ?",
                                     (timeframe, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')))
            rows = await cursor.fetchall()
            columns = [desc[0] for desc in cursor.description]
            df = pd.DataFrame(rows, columns=columns)
            await cursor.close()
            if not df.empty:
                df['date'] = pd.to_datetime(df['date'])
                df = filter_future_dates(df)
                df = fill_missing_values(df)
                # 防護：如果缺少 'n' 或 'vw'，手動設定
                if 'n' not in df.columns:
                    df['n'] = 0
                if 'vw' not in df.columns:
                    df['vw'] = df['close']
                if use_redis and redis_client:
                    try:
                        redis_client.setex(key, 3600, df.to_json())
                        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已將 SQLite 數據快取至 Redis")
                        logging.info(f"Cached SQLite data to Redis: key={key}, timeframe={timeframe}")
                    except redis.RedisError as e:
                        logging.warning(f"無法快取至 Redis: {str(e)}")
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 從 SQLite 快取載入資料：{timeframe}")
                logging.info(f"Loaded data from SQLite: shape={df.shape}, timeframe={timeframe}")
                return df[['date', 'open', 'high', 'low', 'close', 'volume', 'n', 'vw']]
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} SQLite 快取查詢失敗：{str(e)}")
        logging.error(f"SQLite cache query failed: {str(e)}, timeframe={timeframe}, traceback={traceback.format_exc()}")
    # API 呼叫：分批獲取
    apis = [primary_api] + backup_apis
    df_list = []
    for api in apis:
        # 檢查 API 是否支持指定的 timeframe
        if api == 'fixer' and interval['fixer'] is None:
            logging.warning(f"Fixer API 不支持 {timeframe} 時間框架，跳過")
            continue
        proxies = get_proxy(config)
        if not await test_proxy(proxies):
            logging.warning(f"代理不可用，無代理模式")
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 代理不可用，無代理模式")
        current_start = start_date
        while current_start &lt; end_date:
            # 根據 timespan 動態設置分批範圍
            if interval['timespan'] == 'minute':
                batch_end = current_start + timedelta(days=7) # 分鐘級數據分批較小
            elif interval['timespan'] == 'hour':
                batch_end = current_start + timedelta(days=30) # 小時級數據
            else: # day
                batch_end = current_start + timedelta(days=365) # 天級數據
            batch_end = min(batch_end, end_date)
            batch_range = {'start': current_start.strftime('%Y-%m-%d'), 'end': batch_end.strftime('%Y-%m-%d')}
            # 檢查 yfinance 的時間範圍限制
            if api == 'yfinance':
                delta = batch_end - current_start
                if timeframe == '1min' and delta &gt; timedelta(days=7):
                    logging.warning(f"yfinance 不支持 {timeframe} 超過 7 天，跳過此批次")
                    current_start = batch_end
                    continue
                if timeframe == '5min' and delta &gt; timedelta(days=60):
                    logging.warning(f"yfinance 不支持 {timeframe} 超過 60 天，跳過此批次")
                    current_start = batch_end
                    continue
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 分批提取 {api}：{batch_range['start']} 至 {batch_range['end']}")
                logging.info(f"Batch fetch from {api}: {batch_range['start']} to {batch_range['end']}, timeframe={timeframe}")
                try:
                    for attempt in range(5):
                        try:
                            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 提取{api}")
                            ticker = yf.Ticker('USDJPY=X')
                            df = ticker.history(start=batch_range['start'], end=batch_range['end'], interval=interval['yfinance'])
                            if df.empty:
                                logging.warning(f"Yahoo Finance batch empty: timeframe={timeframe}")
                                break
                            df = df.reset_index().rename(columns={'Date': 'date', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'})
                            df['date'] = pd.to_datetime(df['date'])
                            df['n'] = 0 # yfinance 不提供交易次數
                            df['vw'] = df['close'] # 模擬成交量加權平均價格
                            df = fill_missing_values(df)
                            df_list.append(df[['date', 'open', 'high', 'low', 'close', 'volume', 'n', 'vw']])
                            logging.info(f"yfinance data fetched: shape={df.shape}, timeframe={timeframe}")
                            break
                        except Exception as e:
                            if attempt == 4:
                                logging.error(f"Yahoo Finance batch failed: {str(e)}, timeframe={timeframe}, traceback={traceback.format_exc()}")
                                break
                            await asyncio.sleep(2 ** attempt * 4)
                except Exception as e:
                    logging.error(f"{api} API batch failed: {str(e)}, timeframe={timeframe}, traceback={traceback.format_exc()}")
                    continue
            elif api == 'polygon':
                # 根據 polygon 及 timespan 限制動態設置分批範圍
                if interval['timespan'] == 'minute':
                    batch_end = current_start + timedelta(days=30) # 分鐘級數據分批較小
                elif interval['timespan'] == 'hour':
                    batch_end = current_start + timedelta(days=730) # 小時級數據
                else: # day
                    batch_end = current_start + timedelta(days=730) # 天級數據
                batch_end = min(batch_end, end_date)
                batch_range = {'start': current_start.strftime('%Y-%m-%d'), 'end': batch_end.strftime('%Y-%m-%d')}
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 提取{api}")
                api_key = config['api_key'].get('polygon_api_key', '')
                if not api_key:
                    logging.error("Polygon API key not configured")
                    continue
                # 計算預期數據點數以設置 limit
                delta = batch_end - current_start
                limit = 50000
                url = f"https://api.polygon.io/v2/aggs/ticker/C:USDJPY/range/{interval['multiplier']}/{interval['timespan']}/{batch_range['start']}/{batch_range['end']}?adjusted=true&amp;sort=asc&amp;limit={limit}&amp;apiKey={api_key}"
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 提取{api}: {url}")
                logging.info(f"Attempting Polygon API request: {url}, timeframe={timeframe}")

                # 分頁處理：循環請求直到沒有 next_url
                page_count = 0
                batch_results = []  # 收集本批次的 results
                while url:
                    page_count += 1
                    for attempt in range(3):  # 重試 3 次
                        await polygon_rate_limiter.wait()  # 等待速率限制器
                        async with aiohttp.ClientSession() as session:
                            try:
                                async with session.get(url, timeout=10) as response:
                                    data = await response.json()
                                    if data.get('status') not in ['OK', 'DELAYED']:
                                        logging.warning(f"Polygon API failed: status={data.get('status')}, message={data.get('error', 'Unknown error')}, timeframe={timeframe}")
                                        break
                                    if data.get('ticker') != 'C:USDJPY':
                                        logging.warning(f"Polygon API returned unexpected ticker: {data.get('ticker')}, timeframe={timeframe}")
                                        break
                                    if 'results' not in data or not data['results']:
                                        logging.warning(f"Polygon API batch empty: timeframe={timeframe}")
                                        break
                                    logging.info(f"Polygon API page {page_count}: queryCount={data.get('queryCount', 0)}, resultsCount={data.get('resultsCount', 0)}, request_id={data.get('request_id', 'N/A')}, timeframe={timeframe}")
                                    batch_results.extend(data['results'])
                                    # 檢查 next_url，如果存在，附加 apiKey
                                    next_url = data.get('next_url')
                                    if next_url:
                                        url = f"{next_url}&amp;apiKey={api_key}"
                                        logging.info(f"Fetching next page: {url}")
                                    else:
                                        url = None
                                    break  # 成功後跳出重試
                            except Exception as e:
                                logging.warning(f"Polygon request failed on attempt {attempt+1}: {str(e)}")
                                if attempt == 2:
                                    url = None  # 失敗後停止
                                await asyncio.sleep(2 ** attempt * 2)

                # 處理收集的 results
                if batch_results:
                    df_batch = pd.DataFrame(batch_results)[['t', 'o', 'h', 'l', 'c', 'v', 'n', 'vw']].rename(
                        columns={'t': 'date', 'o': 'open', 'h': 'high', 'l': 'low', 'c': 'close', 'v': 'volume', 'n': 'n', 'vw': 'vw'}
                    )
                    df_batch['date'] = pd.to_datetime(df_batch['date'], unit='ms')
                    df_batch = fill_missing_values(df_batch)
                    df_list.append(df_batch)
                    logging.info(f"Polygon data fetched (all pages): shape={df_batch.shape}, pages={page_count}, timeframe={timeframe}")
                else:
                    logging.warning(f"No results from Polygon for batch: {batch_range['start']} to {batch_range['end']}")

            elif api == 'fcs':
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 提取{api}")
                api_key = config['api_key'].get('fcs_api_key', '')
                if not api_key:
                    logging.error("FCS API key not configured")
                    continue
                url = f"https://fcsapi.com/api-v3/forex/history?symbol=USD/JPY&amp;access_key={api_key}&amp;period={interval['fcs']}&amp;from={batch_range['start']}&amp;to={batch_range['end']}"
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=10) as response:
                        data = await response.json()
                        if 'response' not in data or not data['response']:
                            logging.warning(f"FCS API batch empty: timeframe={timeframe}")
                            continue
                        if data.get('code') != 200:
                            logging.warning(f"FCS API failed: code={data.get('code')}, message={data.get('msg', 'Unknown error')}")
                            continue
                        df = pd.DataFrame(data['response'])[['datetime', 'open', 'high', 'low', 'close', 'volume']].rename(columns={'datetime': 'date'})
                        df['date'] = pd.to_datetime(df['date'])
                        df['n'] = 0 # FCS 不提供交易次數
                        df['vw'] = df['close'] # 模擬成交量加權平均價格
                        df = fill_missing_values(df)
                        df_list.append(df)
                        logging.info(f"FCS data fetched: shape={df.shape}, timeframe={timeframe}")
            elif api == 'fixer':
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 提取{api}")
                api_key = config['api_key'].get('fixer_API_Key', '')
                if not api_key:
                    logging.error("Fixer API key not configured")
                    continue
                url = f"http://data.fixer.io/api/timeseries?access_key={api_key}&amp;start_date={batch_range['start']}&amp;end_date={batch_range['end']}&amp;symbols=USD,JPY"
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=10) as response:
                        data = await response.json()
                        if not data.get('success') or 'rates' not in data:
                            logging.warning(f"Fixer API batch empty or failed: timeframe={timeframe}")
                            continue
                        rates = data['rates']
                        df_data = []
                        for date, rate in rates.items():
                            if 'USD' in rate and 'JPY' in rate:
                                usd_jpy = rate['JPY'] / rate['USD']
                                df_data.append({'date': date, 'close': usd_jpy, 'open': usd_jpy, 'high': usd_jpy, 'low': usd_jpy, 'volume': 0, 'n': 0, 'vw': usd_jpy})
                        df = pd.DataFrame(df_data)
                        df['date'] = pd.to_datetime(df['date'])
                        df = fill_missing_values(df)
                        df_list.append(df[['date', 'open', 'high', 'low', 'close', 'volume', 'n', 'vw']])
                        logging.info(f"Fixer data fetched: shape={df.shape}, timeframe={timeframe}")
            current_start = batch_end
        if df_list:
            df = pd.concat(df_list, ignore_index=True).drop_duplicates(subset=['date'])
            df = filter_future_dates(df)
            df = fill_missing_values(df)
            await save_data(df, timeframe, db_path, data_type='ohlc')
            if not os.path.exists(CSV_PATH.parent):
                os.makedirs(CSV_PATH.parent)
            df.to_csv(CSV_PATH, index=False)
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} OHLC 數據已儲存到 CSV：{CSV_PATH}")
            if use_redis and redis_client:
                try:
                    redis_client.setex(key, 3600, df.to_json())
                    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已將 API 數據快取至 Redis")
                    logging.info(f"Cached API data to Redis: key={key}, timeframe={timeframe}")
                except redis.RedisError as e:
                    logging.warning(f"無法快取至 Redis: {str(e)}")
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 從 {api} 獲取資料成功")
            logging.info(f"Successfully fetched data from {api}: shape={df.shape}, timeframe={timeframe}")
            return df[['date', 'open', 'high', 'low', 'close', 'volume', 'n', 'vw']]
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 所有 API 和 CSV 後備失敗")
    logging.error(f"All APIs and CSV fallback failed, timeframe={timeframe}")
    return pd.DataFrame()
def fill_missing_values(df: pd.DataFrame) -&gt; pd.DataFrame:
    """填補缺失值：使用前向填補法，確保數據連續性。"""
    if df.empty:
        return df
    numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'n', 'vw']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = df[col].fillna(method='ffill').fillna(method='bfill')
            if df[col].isna().all():
                df[col] = df[col].fillna(0)  # 若全為 NaN，使用 0 作為預設值
    if 'fed_funds_rate' in df.columns:
        df['fed_funds_rate'] = df['fed_funds_rate'].fillna(method='ffill').fillna(method='bfill')
        if df['fed_funds_rate'].isna().all():
            df['fed_funds_rate'] = df['fed_funds_rate'].fillna(0)  # 聯邦基金利率預設為 0
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已填補缺失值")
    logging.info("Missing values filled using forward and backward fill")
    return df
async def compute_indicators(df: pd.DataFrame, db_path: str, timeframe: str, config: dict = None) -&gt; pd.DataFrame:
    """計算技術指標：使用多線程計算 RSI, MACD, ATR, Stochastic, ADX, Ichimoku, Bollinger, EMA，支援分批計算並檢查重複。"""
    try:
        # 動態數據長度檢查，根據時間框架設置最小數據長度
        min_data_length = {'1min': 200, '5min': 150, '1h': 100, '4h': 60, '1d': 60}
        required_length = min_data_length.get(timeframe, 100)
        if len(df) &lt; required_length:
            logging.warning(f"數據長度不足 ({len(df)} &lt; {required_length}) for timeframe {timeframe}，跳過指標計算")
            return df

        # 檢查 SQLite 是否已存儲最新指標
        async with aiosqlite.connect(db_path, timeout=10) as conn:
            cursor = await conn.execute(
                "SELECT MAX(date) FROM indicators WHERE timeframe = ? AND indicator IN (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)",
                (timeframe, 'RSI', 'MACD', 'ATR', 'Stoch_k', 'ADX', 'Ichimoku_tenkan', 'Ichimoku_kijun', 'BB_upper', 'EMA_12', 'EMA_26')
            )
            last_date = await cursor.fetchone()
            last_date = pd.to_datetime(last_date[0]) if last_date[0] else None
            if last_date and df['date'].max() &lt;= last_date:
                logging.info(f"發現最新指標已存儲 for timeframe {timeframe}，從 SQLite 載入")
                cursor = await conn.execute(
                    "SELECT date, indicator, value FROM indicators WHERE timeframe = ? AND date &gt;= ?",
                    (timeframe, df['date'].min().strftime('%Y-%m-%d'))
                )
                rows = await cursor.fetchall()
                if rows:
                    indicators_df = pd.DataFrame(rows, columns=['date', 'indicator', 'value'])
                    indicators_df = indicators_df.pivot(index='date', columns='indicator', values='value')
                    indicators_df['date'] = pd.to_datetime(indicators_df['date'])
                    df = df.merge(indicators_df, on='date', how='left')
                    df = fill_missing_values(df)
                    return df

        # 分批計算（若數據量大）
        batch_size = 10000 if timeframe in ['1min', '5min'] else None
        if batch_size and len(df) &gt; batch_size:
            logging.info(f"數據量大 ({len(df)})，分批計算，batch_size={batch_size}")
            df_list = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]
        else:
            df_list = [df]

        from concurrent.futures import ThreadPoolExecutor                                                
        result_dfs = []
        for batch_df in df_list:
            def calc_rsi(df):
                rsi = ta.rsi(df['close'], length=14)
                return rsi if rsi is not None else pd.Series(np.nan, index=df.index, name='RSI')

            def calc_macd(df):
                macd = ta.macd(df['close'])
                if macd is None:
                    return pd.DataFrame({'MACD': np.nan, 'MACD_signal': np.nan, 'MACD_hist': np.nan}, index=df.index)
                return macd[['MACD_12_26_9', 'MACDs_12_26_9', 'MACDh_12_26_9']].rename(
                    columns={'MACD_12_26_9': 'MACD', 'MACDs_12_26_9': 'MACD_signal', 'MACDh_12_26_9': 'MACD_hist'}
                )

            def calc_atr(df):
                atr = ta.atr(df['high'], df['low'], df['close'], length=14)
                return atr if atr is not None else pd.Series(np.nan, index=df.index, name='ATR')

            def calc_stoch(df):
                stoch = ta.stoch(df['high'], df['low'], df['close'], k=14, d=3)
                if stoch is None:
                    return pd.Series(np.nan, index=df.index, name='Stoch_k')
                return stoch['STOCHk_14_3_3']

            def calc_adx(df):
                adx = ta.adx(df['high'], df['low'], df['close'], length=14)
                if adx is None:
                    return pd.Series(np.nan, index=df.index, name='ADX')
                return adx['ADX_14']

            def calc_ichimoku(df):
                if not all(col in df.columns for col in ['high', 'low', 'close']):
                    logging.warning(f"Missing required columns for Ichimoku calculation: {df.columns.tolist()}")
                    return pd.DataFrame({
                        'Ichimoku_tenkan': np.nan, 'Ichimoku_kijun': np.nan, 'Ichimoku_span_a': np.nan, 'Ichimoku_span_b': np.nan
                    }, index=df.index)
                if df[['high', 'low', 'close']].isna().any().any():
                    logging.warning(f"NaN values detected in high, low, or close columns for timeframe {timeframe}")
                    df = df.fillna(method='ffill').fillna(method='bfill')
                ich = ta.ichimoku(df['high'], df['low'], df['close'], tenkan=9, kijun=26, senkou=52)
                if ich[0] is None:
                    logging.warning(f"Ichimoku calculation returned None for timeframe {timeframe}")
                    return pd.DataFrame({
                        'Ichimoku_tenkan': np.nan, 'Ichimoku_kijun': np.nan, 'Ichimoku_span_a': np.nan, 'Ichimoku_span_b': np.nan
                    }, index=df.index)
                try:
                    # Replace with actual column names from debug output
                    return ich[0][['ISA_9', 'ISB_26', 'ITS_9', 'IKS_26']].rename(
                        columns={
                            'ITS_9': 'Ichimoku_tenkan',
                            'IKS_26': 'Ichimoku_kijun',
                            'ISA_9': 'Ichimoku_span_a',
                            'ISB_26': 'Ichimoku_span_b'
                        }
                    )
                except KeyError as e:
                    logging.error(f"Ichimoku column error: {str(e)}, timeframe={timeframe}")
                    return pd.DataFrame({
                        'Ichimoku_tenkan': np.nan, 'Ichimoku_kijun': np.nan, 'Ichimoku_span_a': np.nan, 'Ichimoku_span_b': np.nan
                    }, index=df.index)

            def calc_bbands(df):
                bb = ta.bbands(df['close'], length=20, std=2)
                if bb is None:
                    return pd.DataFrame({'BB_upper': np.nan, 'BB_middle': np.nan, 'BB_lower': np.nan}, index=df.index)
                return bb[['BBU_20_2.0', 'BBM_20_2.0', 'BBL_20_2.0']].rename(
                    columns={'BBU_20_2.0': 'BB_upper', 'BBM_20_2.0': 'BB_middle', 'BBL_20_2.0': 'BB_lower'}
                )

            def calc_ema_12(df):
                ema = ta.ema(df['close'], length=12)
                return ema if ema is not None else pd.Series(np.nan, index=df.index, name='EMA_12')

            def calc_ema_26(df):
                ema = ta.ema(df['close'], length=26)
                return ema if ema is not None else pd.Series(np.nan, index=df.index, name='EMA_26')

            with ThreadPoolExecutor(max_workers=8) as executor:
                futures = [
                    executor.submit(calc_rsi, batch_df),
                    executor.submit(calc_macd, batch_df),
                    executor.submit(calc_atr, batch_df),
                    executor.submit(calc_stoch, batch_df),
                    executor.submit(calc_adx, batch_df),
                    executor.submit(calc_ichimoku, batch_df),
                    executor.submit(calc_bbands, batch_df),
                    executor.submit(calc_ema_12, batch_df),
                    executor.submit(calc_ema_26, batch_df)
                ]
                results = [f.result() for f in futures]

            batch_df = batch_df.copy()
            batch_df['RSI'] = results[0]
            batch_df[['MACD', 'MACD_signal', 'MACD_hist']] = results[1]
            batch_df['ATR'] = results[2]
            batch_df['Stoch_k'] = results[3]
            batch_df['ADX'] = results[4]
            batch_df[['Ichimoku_tenkan', 'Ichimoku_kijun', 'Ichimoku_span_a', 'Ichimoku_span_b']] = results[5]
            batch_df['Ichimoku_cloud_top'] = results[5][['Ichimoku_span_a', 'Ichimoku_span_b']].max(axis=1)
            batch_df[['BB_upper', 'BB_middle', 'BB_lower']] = results[6]
            batch_df['EMA_12'] = results[7]
            batch_df['EMA_26'] = results[8]

            # 改進缺失值處理：前向填補代替 dropna
            batch_df = fill_missing_values(batch_df)
            result_dfs.append(batch_df)

        df = pd.concat(result_dfs, ignore_index=True).drop_duplicates(subset=['date'])

        # 存入 DB
        await save_data(df, timeframe, db_path, data_type='indicators')
        logging.info(f"指標數據已存入 DB: timeframe={timeframe}")

        # 存入 CSV
        if config:
            indicators_csv_path = Path(config['system_config']['root_dir']) / 'data' / f'usd_jpy_{timeframe}_indicators.csv'
            if not os.path.exists(indicators_csv_path.parent):
                os.makedirs(indicators_csv_path.parent)
            df.to_csv(indicators_csv_path, index=False)
            logging.info(f"指標數據已存入 CSV: {indicators_csv_path}")

        return df
    except Exception as e:
        logging.error(f"指標計算錯誤: {str(e)}, timeframe={timeframe}, traceback={traceback.format_exc()}")
        return df
async def fetch_economic_calendar(date_range: dict, db_path: str, config: dict) -&gt; pd.DataFrame:
    """獲取經濟日曆數據並儲存到 SQLite，新增 FRED API 獲取聯邦基金利率。"""
    use_redis = config.get('system_config', {}).get('use_redis', True)
   
    redis_client = get_redis_client(config)
    key = 'usd_jpy_economic_calendar'
    CSV_PATH = Path(config['system_config']['root_dir']) / 'data' / 'economic_calendar.csv'
    start_date = pd.to_datetime(date_range['start']) - timedelta(days=7)
    end_date = pd.to_datetime(date_range['end']) + timedelta(days=7)
    # 第一層快取：Redis
    if use_redis and redis_client:
        try:
            cached = redis_client.get(key)
            if cached:
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 從 Redis 快取載入經濟日曆數據")
                logging.info(f"Loaded economic calendar from Redis cache: key={key}")
                df = pd.read_json(cached)
                df['date'] = pd.to_datetime(df['date'])
                df = fill_missing_values(df)
                return df[['date', 'event', 'impact', 'fed_funds_rate']]
        except redis.RedisError as e:
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} Redis 快取查詢失敗：{str(e)}")
            logging.warning(f"Redis cache query failed: {str(e)}, falling back to SQLite", extra={'mode': 'fetch_economic_calendar'})
    # 第二層快取：SQLite
    try:
        async with aiosqlite.connect(db_path, timeout=10) as conn:
            cursor = await conn.execute("SELECT * FROM economic_calendar WHERE date BETWEEN ? AND ?",
                                     (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')))
            rows = await cursor.fetchall()
            columns = [desc[0] for desc in cursor.description]
            df = pd.DataFrame(rows, columns=columns)
            await cursor.close()
            if not df.empty:
                df['date'] = pd.to_datetime(df['date'])
                df = filter_future_dates(df)
                df = fill_missing_values(df)
                if use_redis and redis_client:
                    try:
                        redis_client.setex(key, 3600, df.to_json())
                        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已將 SQLite 數據快取至 Redis")
                        logging.info(f"Cached SQLite data to Redis: key={key}")
                    except redis.RedisError as e:
                        logging.warning(f"無法快取至 Redis: {str(e)}")
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 從 SQLite 載入經濟日曆數據")
                logging.info(f"Loaded economic calendar from SQLite: shape={df.shape}")
                return df[['date', 'event', 'impact', 'fed_funds_rate']]
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} SQLite 經濟日曆查詢失敗：{str(e)}")
        logging.error(f"SQLite economic calendar query failed: {str(e)}, traceback={traceback.format_exc()}")
    # 使用 investpy 獲取經濟日曆數據
    try:
        proxies = get_proxy(config)
        if not await test_proxy(proxies):
            logging.warning("主代理不可用，嘗試備用代理")
            for backup_proxy in config.get('system_config', {}).get('backup_proxies', []):
                if await test_proxy(backup_proxy):
                    proxies = backup_proxy
                    break
            else:
                proxies = {}
                logging.warning("所有代理不可用，無代理模式")
        importances = ['high', 'medium']
        time_zone = 'GMT +8:00'
        from_date = start_date.strftime('%d/%m/%Y')
        to_date = end_date.strftime('%d/%m/%Y')
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 正在從 investpy 獲取經濟日曆數據：{from_date} 至 {to_date}")
        logging.info(f"Fetching economic calendar from investpy: start={from_date}, end={to_date}")
        calendar = investpy.economic_calendar(
            importances=importances,
            time_zone=time_zone,
            from_date=from_date,
            to_date=to_date,
            countries=['united states', 'japan']
        )
        if calendar.empty:
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} investpy 經濟日曆數據為空")
            logging.warning("investpy economic calendar data is empty")
            df = pd.DataFrame()
        else:
            calendar['date'] = pd.to_datetime(calendar['date'], format='%d/%m/%Y')
            calendar = calendar[calendar['importance'].notnull()]
            calendar['event'] = calendar['currency'] + ' ' + calendar['event']
            calendar['impact'] = calendar['importance'].str.capitalize()
            df = calendar[['date', 'event', 'impact']]
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} investpy 經濟日曆數據獲取失敗：{str(e)}")
        logging.error(f"Failed to fetch investpy economic calendar: {str(e)}, traceback={traceback.format_exc()}")
        df = pd.DataFrame()
    # 使用 FRED API 獲取聯邦基金利率
    try:
        fred_api_key = config['api_key'].get('fred_api_key', '')
        if not fred_api_key:
            logging.error("FRED API key not configured")
        else:
            url = f"https://api.stlouisfed.org/fred/series/observations?series_id=FEDFUNDS&amp;api_key={fred_api_key}&amp;file_type=json&amp;observation_start={start_date.strftime('%Y-%m-%d')}&amp;observation_end={end_date.strftime('%Y-%m-%d')}"
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    data = await response.json()
                    if 'observations' not in data:
                        logging.warning("FRED API returned no observations")
                    else:
                        fred_data = pd.DataFrame(data['observations'])[['date', 'value']].rename(columns={'value': 'fed_funds_rate'})
                        fred_data['date'] = pd.to_datetime(fred_data['date'])
                        fred_data['fed_funds_rate'] = fred_data['fed_funds_rate'].astype(float)
                        if not df.empty:
                            df = df.merge(fred_data[['date', 'fed_funds_rate']], on='date', how='left')
                        else:
                            df = fred_data[['date', 'fed_funds_rate']]
                            df['event'] = 'FEDFUNDS'
                            df['impact'] = 'High'
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} FRED API 獲取失敗：{str(e)}")
        logging.error(f"Failed to fetch FRED API data: {str(e)}, traceback={traceback.format_exc()}")
    # 儲存數據前去重，防止UNIQUE constraint failed
    if not df.empty:
        df = df.drop_duplicates(subset=['date', 'event'], keep='last')  # 去重，保留最後一筆
        df = filter_future_dates(df)
        df = fill_missing_values(df)
        await save_data(df, timeframe='1 day', db_path=db_path, data_type='economic')
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 經濟日曆數據已儲存到 SQLite")
        logging.info(f"Economic calendar data saved to SQLite: shape={df.shape}")
        if not os.path.exists(CSV_PATH.parent):
            os.makedirs(CSV_PATH.parent)
        df.to_csv(CSV_PATH, index=False)
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 經濟日曆數據已儲存到 CSV：{CSV_PATH}")
        logging.info(f"Economic calendar data saved to CSV: {CSV_PATH}")
        if use_redis and redis_client:
            try:
                redis_client.setex(key, 3600, df.to_json())
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已將經濟日曆數據快取至 Redis")
                logging.info(f"Cached economic calendar data to Redis: key={key}")
            except redis.RedisError as e:
                logging.warning(f"無法快取至 Redis: {str(e)}")
        return df[['date', 'event', 'impact', 'fed_funds_rate']]
    # 從 CSV 載入備份
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 無 investpy/FRED 數據，嘗試從 CSV 載入")
    logging.info("No investpy/FRED data, trying to load from CSV")
    if CSV_PATH.exists():
        df = pd.read_csv(CSV_PATH)
        required_columns = ['date', 'event', 'impact']
        if all(col in df.columns for col in required_columns):
            df['date'] = pd.to_datetime(df['date'])
            df = df[(df['date'] &gt;= start_date) &amp; (df['date'] &lt;= end_date)]
            df = df.drop_duplicates(subset=['date', 'event'], keep='last')  # 去重
            df = filter_future_dates(df)
            df = fill_missing_values(df)
            await save_data(df, timeframe='1 day', db_path=db_path, data_type='economic')
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 從 CSV 載入經濟日曆數據並儲存到 SQLite")
            logging.info(f"Loaded and saved economic calendar from CSV: shape={df.shape}")
            if use_redis and redis_client:
                try:
                    redis_client.setex(key, 3600, df.to_json())
                    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已將 CSV 數據快取至 Redis")
                    logging.info(f"Cached CSV data to Redis: key={key}")
                except redis.RedisError as e:
                    logging.warning(f"無法快取至 Redis: {str(e)}")
            return df[['date', 'event', 'impact', 'fed_funds_rate']]
        else:
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} CSV 檔案缺少必要欄位：{required_columns}")
            logging.warning(f"Invalid or missing columns in {CSV_PATH}, columns={df.columns.tolist()}")
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 警告：經濟日曆數據為空")
    logging.warning("Economic calendar is empty")
    return pd.DataFrame()</code></pre></li>

        <li><h3>File: .\main.py</h3>
        <pre><code>import asyncio
import logging
import logging.handlers
import pandas as pd
import torch
import json
import os
import time
from datetime import datetime, timedelta
from dotenv import load_dotenv
from data_acquisition import fetch_data, compute_indicators, fetch_economic_calendar
from ai_models import update_model, predict_sentiment, integrate_sentiment
from trading_strategy import ForexEnv, train_ppo, make_decision, backtest, connect_ib, execute_trade
from risk_management import calculate_stop_loss, calculate_take_profit, calculate_position_size, predict_volatility
from utils import check_hardware, setup_proxy, check_volatility, save_data, save_periodically, initialize_db, load_settings, decrypt_key
import streamlit as st
from prometheus_client import Counter, Histogram
from pathlib import Path
# Prometheus 指標，用於監控交易次數和 API 延遲
trade_counter = Counter('usd_jpy_trades_total', 'Total number of trades executed', ['action', 'mode'])
api_latency = Histogram('usd_jpy_api_latency_seconds', 'API call latency', ['mode'])
# 結構化 JSON 日誌格式，方便後續分析
class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'filename': record.filename,
            'funcName': record.funcName,
            'mode': getattr(record, 'mode', 'unknown')
        }
        return json.dumps(log_data, ensure_ascii=False)
# 配置日誌，區分回測和實時模式
def setup_logging(mode: str):
    """設置日誌：根據模式（回測/實時）創建不同的日誌檔案，並使用 JSON 格式。
    邏輯：每天輪替日誌檔案，保留 7 天備份，確保日誌結構化且易於解析。
    """
    log_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)
    log_file = log_dir / f'app_{mode}_{log_time}.log'
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    handler = logging.handlers.TimedRotatingFileHandler(log_file, when='midnight', backupCount=7)
    handler.setFormatter(JsonFormatter())
    logger.handlers = [handler]
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 日誌設定完成")
    logging.info(f"Logging setup completed for mode: {mode}", extra={'mode': mode})
# 清理舊備份檔案
def clean_old_backups(root_dir: str, days_to_keep: int = 7):
    """清理舊備份檔案：僅保留指定天數的資料庫備份檔案。
    邏輯：遍歷備份目錄，刪除早於指定天數的檔案，確保磁碟空間不被過度佔用。
    """
    backup_dir = Path(root_dir) / 'backups'
    if not backup_dir.exists():
        return
    cutoff_date = datetime.now() - timedelta(days=days_to_keep)
    for backup_file in backup_dir.glob('trading_data_*.db'):
        file_date = datetime.strptime(backup_file.stem.split('_')[-1], '%Y%m%d')
        if file_date &lt; cutoff_date:
            backup_file.unlink()
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 舊備份已刪除")
            logging.info(f"Deleted old backup file: {backup_file}", extra={'mode': 'cleanup'})
# 載入配置檔並設置環境變數
def load_config():
    """載入配置檔：從環境變數和 JSON 檔案載入配置，確保安全性。
    邏輯：優先從環境變數載入加密密鑰，然後解密 API 密鑰，確保敏感資訊不硬編碼。
    """
    load_dotenv()
    config = load_settings()
    api_key = config.get('api_key', {})
    for k in api_key:
        if isinstance(api_key[k], bytes):
            api_key[k] = decrypt_key(api_key[k])
    system_config = config.get('system_config', {})
    trading_params = config.get('trading_params', {})
    fernet_key = api_key.get('FERNET_KEY', '')
    if not fernet_key:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 環境變數未設置")
        logging.error("FERNET_KEY environment variable not set", extra={'mode': 'config'})
        raise ValueError("FERNET_KEY environment variable not set")
    return config, api_key, system_config, trading_params
async def main(mode: str = 'backtest'):
    """主程式入口：協調資料獲取、模型訓練、交易決策和風險管理。
    參數：
        mode: 'backtest' 或 'live'，決定運行回測或實時交易模式。
    邏輯：
        1. 設置日誌和代理，初始化資料庫。
        2. 獲取多時間框架資料和經濟日曆。
        3. 檢查波動性，更新模型，進行情緒分析。
        4. 根據模式執行回測或實時交易。
        5. 定期保存數據並清理舊備份。
    """
    # 設置日誌
    setup_logging(mode)
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 程式啟動中")
    logging.info(f"Starting program in {mode} mode", extra={'mode': mode})
    # 載入配置
    try:
        config, api_key, system_config, trading_params = load_config()
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 設定載入成功")
        logging.info("Configuration loaded successfully", extra={'mode': mode})
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 設定載入失敗")
        logging.error(f"Failed to load configuration: {str(e)}", extra={'mode': mode})
        return
    # 設置代理（僅執行一次）
    setup_proxy()
    device_config, onnx_session = check_hardware()
    db_path = system_config['db_path']
    await initialize_db(db_path)
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 資料庫初始化完成")
    logging.info("Database initialized", extra={'mode': mode})
    # 定義日期範圍
    date_range = {
        'start': (datetime.now() - pd.Timedelta(days=trading_params['min_backtest_days'])).strftime('%Y-%m-%d'),
        'end': datetime.now().strftime('%Y-%m-%d')
    }
    # 獲取多時間框架資料
    timeframes = ['1h', '4h', '1d'] # 修正時間框架
    data_frames = {}
    tasks = []
    for tf in timeframes:
        start_time = time.time()
        print(f"獲取 {tf} 資料中")
        df = await fetch_data(
            primary_api=system_config['data_source'],
            backup_apis=['yfinance', 'fcs'],
            date_range=date_range,
            timeframe=tf,
            db_path=db_path,
            config=config
        )
        api_latency.labels(mode=mode).observe(time.time() - start_time)
        if df.empty:
            print(f"{tf} 資料獲取失敗")
            logging.error(f"Failed to fetch {tf} data", extra={'mode': mode})
            for task in tasks:
                task.cancel()
            return
        # 傳遞 db_path, timeframe, config 給 compute_indicators，讓其內部存入 DB 和 CSV
        df = await compute_indicators(df, db_path, tf, config)
        data_frames[tf] = df
        print(f"{tf} 資料處理完成")
        logging.info(f"{tf} data preprocessing completed", extra={'mode': mode})

    # 日期對齊：確保所有時間框架的數據日期範圍一致
    common_dates = None
    for tf in timeframes:
        if not data_frames[tf].empty:
            dates = set(data_frames[tf]['date'])
            common_dates = dates if common_dates is None else common_dates.intersection(dates)
    if common_dates:
        for tf in timeframes:
            if not data_frames[tf].empty:
                data_frames[tf] = data_frames[tf][data_frames[tf]['date'].isin(common_dates)].copy()
                logging.info(f"Aligned {tf} data to common dates, rows={len(data_frames[tf])}")
    
    # 獲取經濟日曆
    economic_calendar = await fetch_economic_calendar(date_range, db_path, config)
    if not economic_calendar.empty:
        logging.info("Economic calendar is not empty", extra={'mode': mode})
        data_frames['1d'] = data_frames['1d'].merge(
            economic_calendar[['date', 'event', 'impact', 'fed_funds_rate']], on='date', how='left'
        )
        data_frames['1d']['impact'] = data_frames['1d']['impact'].fillna('Low')
    # 啟動定期儲存任務
    for tf in timeframes:
        if not data_frames[tf].empty:
            tasks.append(asyncio.create_task(save_periodically(data_frames[tf], tf, db_path, system_config['root_dir'], data_type='ohlc')))
            tasks.append(asyncio.create_task(save_periodically(data_frames[tf], tf, db_path, system_config['root_dir'], data_type='indicators')))
    if not economic_calendar.empty:
        tasks.append(asyncio.create_task(save_periodically(economic_calendar, '1d', db_path, system_config['root_dir'], data_type='economic')))
    # 清理舊備份
    clean_old_backups(system_config['root_dir'])
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 備份清理完成")
    logging.info("Old backup files cleaned", extra={'mode': mode})
    # 檢查波動性
    if '1h' not in data_frames or data_frames['1h'].empty or 'ATR' not in data_frames['1h'].columns:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 1小时数据或ATR列缺失，使用默认会话模式 'normal'")
        logging.warning("1h data or ATR column missing, defaulting to 'normal' session", extra={'mode': mode})
        session = 'normal'
    else:
        if not check_volatility(data_frames['1h']['ATR'].mean(), threshold=trading_params['atr_threshold']):
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 高波动，暂停执行")
            logging.warning("High volatility detected, halting execution", extra={'mode': mode})
            for task in tasks:
                task.cancel()
            return
    # 模型更新與訓練
    session = 'high_volatility' if data_frames['1h']['ATR'].iloc[-1] &gt; trading_params['atr_threshold'] else 'normal'
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 更新模型中")
    models = update_model(data_frames['1d'], 'models', session, device_config)
    if not models:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 模型更新失敗")
        logging.error("Model update failed", extra={'mode': mode})
        for task in tasks:
            task.cancel()
        return
    # 情緒分析整合
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 進行情緒分析中")
    sentiment_score = await predict_sentiment(date_range['end'], db_path, config)
    sentiment_adjustment = integrate_sentiment(sentiment_score)
    print(f"情緒分析完成，分數={sentiment_score:.2f}")
    logging.info(f"Sentiment analysis result: score={sentiment_score}, adjustment={sentiment_adjustment}", extra={'mode': mode})
    # 準備交易環境與 PPO
    env = ForexEnv(data_frames)
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 訓練 PPO 模型中")
    ppo_model = train_ppo(env, device_config)
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} PPO 訓練完成")
    logging.info("PPO model training completed", extra={'mode': mode})
    # 交易決策
    action = make_decision(ppo_model, data_frames, sentiment_score)
    print(f"交易決策：{action}")
    logging.info(f"Trading decision: {action}", extra={'mode': mode})
    # 風險管理
    if '1h' not in data_frames or data_frames['1h'].empty or 'ATR' not in data_frames['1h'].columns or 'close' not in data_frames['1h'].columns:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 1小时数据或必要列缺失，无法进行风险管理")
        logging.error("1h data or required columns missing, cannot proceed with risk management", extra={'mode': mode})
        for task in tasks:
            task.cancel()
        return
    atr = data_frames['1h']['ATR'].iloc[-1]
    predicted_vol = predict_volatility(data_frames['1h'], model_path='models/volatility_model.pkl')
    current_price = data_frames['1h']['close'].iloc[-1]
    stop_loss = calculate_stop_loss(current_price, atr, action)
    take_profit = calculate_take_profit(current_price, atr, action)
    position_size = await calculate_position_size(trading_params['capital'], trading_params['risk_percent'], current_price - stop_loss, sentiment_score, db_path)
    logging.info(f"Action: {action}, Stop Loss: {stop_loss}, Take Profit: {take_profit}, Position Size: {position_size}", extra={'mode': mode})
    trade = {'action': action, 'price': current_price, 'quantity': position_size, 'stop_loss': stop_loss, 'take_profit': take_profit, 'leverage': 1}
    if not compliance_check(trade):
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 交易不符合規定")
        logging.warning("Trade does not comply with leverage limits", extra={'mode': mode})
        for task in tasks:
            task.cancel()
        return
    # 根據模式執行回測或實時交易
    if mode == 'backtest':
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 執行回測中")
        result = backtest(data_frames['1d'], lambda x: make_decision(ppo_model, data_frames, sentiment_score), initial_capital=trading_params['capital'])
        logging.info(f"Backtest Results: {result}", extra={'mode': mode})
        report_dir = Path('reports')
        report_dir.mkdir(exist_ok=True)
        pd.DataFrame([result]).to_csv(report_dir / f'backtest_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv', index=False)
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 回測完成")
        logging.info("Backtest completed, report generated", extra={'mode': mode})
    elif mode == 'live':
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 啟動實時交易")
        ib = connect_ib()
        trade_counter.labels(action=action, mode=mode).inc()
        execute_trade(ib, action, current_price, position_size, stop_loss, take_profit)
        st.title("USD/JPY 交易儀表板")
        st.line_chart(data_frames['1h']['close'])
        st.write("最新決策:", action)
        st.write("最新指標:", data_frames['1h'][['RSI', 'MACD', 'Stoch_k', 'ADX', 'BB_upper', 'BB_lower', 'EMA_12', 'EMA_26']].iloc[-1])
        override_action = st.selectbox("手動覆寫決策", ["無", "買入", "賣出", "持有"])
        if override_action != "無":
            action = override_action
            logging.info(f"User overridden decision: {action}", extra={'mode': mode})
            trade_counter.labels(action=action, mode=mode).inc()
            execute_trade(ib, action, current_price, position_size, stop_loss, take_profit)
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 實時交易執行中")
        logging.info("Live trading mode running", extra={'mode': mode})
    # 清理任務
    for task in tasks:
        task.cancel()
    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 程式執行完畢")
    logging.info("Program execution completed", extra={'mode': mode})
def compliance_check(trade: dict) -&gt; bool:
    """檢查交易是否符合槓桿限制。
    邏輯：確保槓桿不超過 30:1，符合監管要求。
    """
    leverage = trade.get('leverage', 1)
    is_compliant = leverage &lt;= 30
    logging.info(f"Leverage check: leverage={leverage}, compliant={is_compliant}", extra={'mode': 'compliance'})
    return is_compliant
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="USD/JPY 自動交易系統")
    parser.add_argument('--mode', choices=['backtest', 'live'], default='backtest', help="運行模式：回測或實時交易")
    args = parser.parse_args()
    asyncio.run(main(args.mode))</code></pre></li>

        <li><h3>File: .\risk_management.py</h3>
        <pre><code>import xgboost as xgb
import pandas as pd
import logging
from psutil import virtual_memory, cpu_percent
import onnxruntime as ort
import numpy as np
from pathlib import Path
import aiosqlite
from datetime import datetime
from ai_models import FEATURES
async def get_current_exposure(db_path: str) -&gt; float:
    """獲取當前總持倉暴露（以美元計）。"""
    # 函數說明：從資料庫查詢當前持倉總暴露值。
    try:
        async with aiosqlite.connect(db_path, timeout=10) as conn:
            cursor = await conn.execute("SELECT SUM(SUM(volume * price)) as total_exposure FROM trades WHERE action IN ('買入', '賣出')")
            result = await cursor.fetchone()
            total_exposure = result[0] or 0.0
            logging.info(f"當前總持倉暴露: {total_exposure}")
            return total_exposure
    except Exception as e:
        logging.error(f"獲取持倉暴露失敗: {e}")
        return 0.0
def calculate_stop_loss(current_price: float, atr: float, action: str, multiplier: float = 2) -&gt; float:
    """計算止損：基於 ATR 動態調整，區分多頭和空頭。"""
    # 函數說明：根據 ATR 和交易方向計算止損價格。
    try:
        if action == "買入":
            stop_loss = current_price - (multiplier * atr)
        elif action == "賣出":
            stop_loss = current_price + (multiplier * atr)
        else:
            stop_loss = current_price
        logging.info(f"計算止損: 當前價格={current_price}, ATR={atr}, 行動={action}, 止損={stop_loss}")
        return stop_loss
    except Exception as e:
        logging.error(f"止損計算錯誤: {e}")
        return current_price
def calculate_take_profit(current_price: float, atr: float, action: str, multiplier: float = 2) -&gt; float:
    """計算止盈：基於 ATR 動態調整，區分多頭和空頭。"""
    # 函數說明：根據 ATR 和交易方向計算止盈價格。
    try:
        if action == "買入":
            take_profit = current_price + (multiplier * atr)
        elif action == "賣出":
            take_profit = current_price - (multiplier * atr)
        else:
            take_profit = current_price
        logging.info(f"計算止盈: 當前價格={current_price}, ATR={atr}, 行動={action}, 止盈={take_profit}")
        return take_profit
    except Exception as e:
        logging.error(f"止盈計算錯誤: {e}")
        return current_price
def check_resources(threshold_mem: float = 0.9, threshold_cpu: float = 80.0) -&gt; bool:
    """檢查系統資源：確保記憶體和 CPU 使用率不過高，記憶體閾值調整為 90%。"""
    # 函數說明：檢查系統資源使用率，若超過閾值則返回 False。
    try:
        mem = virtual_memory()
        cpu = cpu_percent(interval=1)
        if mem.percent &gt; threshold_mem * 100 or cpu &gt; threshold_cpu:
            logging.warning(f"資源使用過高：記憶體 {mem.percent}%，CPU {cpu}%")
            return False
        logging.info(f"資源檢查通過：記憶體 {mem.percent}%，CPU {cpu}%")
        return True
    except Exception as e:
        logging.error(f"資源檢查錯誤: {e}")
        return False
async def calculate_position_size(capital: float, risk_percent: float, stop_loss_distance: float, sentiment: float = 0.0, db_path: str = "C:\\Trading\\data\\trading_data.db") -&gt; float:
    """計算倉位大小：控制風險，檢查總持倉暴露（5% 資本）及槓桿限制（30:1）。"""
    # 函數說明：計算交易倉位大小，考慮風險百分比、情緒調整和暴露限額。
    try:
        if abs(sentiment) &gt; 0.8:
            logging.warning(f"極端情緒分數: {sentiment}，倉位大小設為 0")
            return 0.0
        # 關鍵邏輯：計算基礎倉位並根據情緒調整。
        base_size = (capital * risk_percent) / stop_loss_distance if stop_loss_distance &gt; 0 else 0
        adjustment = 1.2 if sentiment &gt; 0.4 else 0.8 if sentiment &lt; -0.4 else 1.0
        position_size = base_size * adjustment
        # 檢查總持倉暴露
        total_exposure = await get_current_exposure(db_path)
        max_exposure = capital * 0.05 # 最大暴露限額為資本的 5%
        if total_exposure + (position_size * stop_loss_distance) &gt; max_exposure:
            logging.warning(f"超過最大暴露限額: 當前={total_exposure}, 擬新增={position_size * stop_loss_distance}, 限額={max_exposure}")
            return 0.0
        # 檢查槓桿限制
        leverage = (position_size * stop_loss_distance) / capital if capital &gt; 0 else 0
        if leverage &gt; 30:
            logging.warning(f"槓桿超過 30:1: 計算槓桿={leverage:.2f}")
            return 0.0
        logging.info(f"計算倉位大小：基礎={base_size:.2f}，情緒調整={adjustment:.2f}，最終={position_size:.2f}，槓桿={leverage:.2f}")
        return position_size
    except Exception as e:
        logging.error(f"倉位計算錯誤: {e}")
        return 0
def predict_volatility(df: pd.DataFrame, model_path: str = 'models/lightgbm_model_quantized.onnx') -&gt; float:
    """預測波動性：使用 ONNX LightGBM 模型預測 ATR。"""
    # 函數說明：使用 ONNX 模型預測未來波動性，若失敗則回退到平均 ATR。
    try:
        X = df[FEATURES].iloc[-1:].values.astype(np.float32)
        if len(X) == 0:
            logging.error("X 數據為空，回退到平均 ATR")
            return df['ATR'].mean()
        model_dir = Path(model_path).parent
        model_dir.mkdir(exist_ok=True)
        session = ort.InferenceSession(model_path, providers=['VitisAIExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])
        pred = session.run(None, {'input': X})[0][0]
        logging.info(f"LightGBM 波動性預測: {pred}")
        return pred
    except Exception as e:
        logging.error(f"波動預測錯誤: {e}")
        return df['ATR'].mean()</code></pre></li>

        <li><h3>File: .\trading_strategy.py</h3>
        <pre><code>from stable_baselines3 import PPO
import gymnasium as gym
import numpy as np
import pandas as pd
from ib_insync import IB, Forex, BracketOrder
import logging
from risk_management import check_resources, calculate_stop_loss, calculate_position_size
from utils import load_settings
from ai_models import FEATURES                     
class ForexEnv(gym.Env):
    """外匯環境：用於 PPO 強化學習訓練，支援多時間框架。"""
    # 類別說明：自定義 Gym 環境，用於模擬外匯交易，支援多時間框架觀察空間。
    def __init__(self, data_frames: dict, spread: float = 0.0002):
        super().__init__()
        tf_mapping = {'1 hour': '1h', '4 hours': '4h', '1 day': '1d'}
        self.data_frames = {tf_mapping.get(k, k): v for k, v in data_frames.items()}
        self.spread = spread
        self.current_step = 0
        self.action_space = gym.spaces.Discrete(3) # 買, 賣, 持
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(27,), dtype=np.float32
        ) # 9 features × 3 timeframes 1h/4h/daily 的 5 個指標
    def reset(self):
        self.current_step = 0
        return self._get_obs()
    def _get_obs(self):
        # 關鍵邏輯：從多時間框架提取觀察值，若數據不足則填充 0。
        obs = []
        for tf in ['1h', '4h', '1d']:
            df = self.data_frames[tf]
            if self.current_step &lt; len(df):
                obs.extend(df[FEATURES[:-1]].iloc[self.current_step].values)  # 排除fed_funds_rate如果不需要
            else:
                obs.extend([0] * 9) # 填充 0 以保持形狀一致
        return np.array(obs, dtype=np.float32)
    def step(self, action):
        price = self.data_frames['1h']['close'].iloc[self.current_step]
        reward = 0
        if action == 0: # 買
            reward -= self.spread
        elif action == 1: # 賣
            reward -= self.spread
        self.current_step += 1
        done = self.current_step &gt;= min(len(self.data_frames[tf]) for tf in self.data_frames) - 1
        return self._get_obs(), reward, done, {}
def train_ppo(env, device_config: dict = None):
    """訓練 PPO：強化學習優化交易決策。，從 config 載入參數。
    邏輯：使用 MlpPolicy，根據配置的步數學習，保存模型。
    """
    # 函數說明：訓練 PPO 模型，用於強化學習決策優化。
    try:
        config = load_settings() # 載入配置
        total_timesteps = config.get('trading_params', {}).get('ppo_timesteps', 1000) # 從 config 獲取，若無則預設 1000
        learning_rate = config.get('trading_params', {}).get('ppo_learning_rate', 0.0003)# 從 config 獲取，若無則預設 0.0003
        device = device_config.get('ppo', torch.device('cpu')) if device_config else torch.device('cpu')
        logging.info(f"PPO 訓練：使用 total_timesteps={total_timesteps}, learning_rate={learning_rate}, device={device}")
        model = PPO("MlpPolicy", env, verbose=0, learning_rate=learning_rate, device=device)
        model.learn(total_timesteps=total_timesteps)
        model.save("models/ppo_model")
        logging.info("PPO 模型訓練完成")
        return model
    except Exception as e:
        logging.error(f"PPO 訓練錯誤: {e}")
        return None
def make_decision(model, data_frames: dict, sentiment: float) -&gt; str:
    """產生決策：結合多時間框架技術指標和情緒分數。"""
    # 函數說明：結合技術指標、情緒分數和 PPO 模型產生買賣持倉決策。
    try:
        # 檢查系統資源
        if not check_resources():
            logging.warning("資源不足，暫停交易")
            return "持有"
        # 多框架技術指標邏輯
        buy_signals = []
        sell_signals = []
        for tf in ['1h', '4h', '1d']:
            df = data_frames[tf]
            if df.empty:
                continue
            rsi = df['RSI'].iloc[-1]
            macd = df['MACD'].iloc[-1]
            macd_signal = df['MACD_signal'].iloc[-1]
            stoch_k = df['Stoch_k'].iloc[-1]
            adx = df['ADX'].iloc[-1]
            # 更新 Ichimoku 信號：考慮 Tenkan-sen 和 Kijun-sen
            ichimoku_buy = (df['Ichimoku_tenkan'].iloc[-1] &gt; df['Ichimoku_kijun'].iloc[-1] and
                           df['close'].iloc[-1] &gt; df['Ichimoku_cloud_top'].iloc[-1])
            ichimoku_sell = (df['Ichimoku_tenkan'].iloc[-1] &lt; df['Ichimoku_kijun'].iloc[-1] and
                            df['close'].iloc[-1] &lt; df['Ichimoku_cloud_top'].iloc[-1])
            bb_signal = df['close'].iloc[-1] &lt; df['BB_lower'].iloc[-1]
            ema_signal = df['EMA_12'].iloc[-1] &gt; df['EMA_26'].iloc[-1]
									  
            economic_impact = df['impact'].iloc[-1] if 'impact' in df.columns and not pd.isna(df['impact'].iloc[-1]) else 'Low'
            economic_pause = economic_impact in ['High', 'Medium']
            buy_signals.append(rsi &lt; 30 and macd &gt; macd_signal and stoch_k &lt; 20 and adx &gt; 25 and ichimoku_buy and bb_signal and ema_signal and not economic_pause)
            sell_signals.append(rsi &gt; 70 and macd &lt; macd_signal and stoch_k &gt; 80 and adx &gt; 25 and ichimoku_sell and df['close'].iloc[-1] &gt; df['BB_upper'].iloc[-1] and df['EMA_12'].iloc[-1] &lt; df['EMA_26'].iloc[-1] and not economic_pause)
        # 關鍵邏輯：計算買賣信號分數並根據情緒調整。
        buy_score = sum(1 for s in buy_signals if s) / len(buy_signals) if buy_signals else 0
        sell_score = sum(1 for s in sell_signals if s) / len(sell_signals) if sell_signals else 0
        # 情緒調整
        if abs(sentiment) &gt; 0.8:
            logging.warning(f"極端情緒分數: {sentiment}，暫停交易")
            return "持有"
        sentiment_adjust = 0.2 if sentiment &gt; 0.4 else -0.2 if sentiment &lt; -0.4 else 0.0
        buy_score += sentiment_adjust
        sell_score -= sentiment_adjust
        # PPO 決策
        obs = []
        for tf in ['1h', '4h', '1d']:
            df = data_frames.get(tf, pd.DataFrame())
            if not df.empty:
                obs.extend(df[FEATURES[:-1]].iloc[-1].values)  # 排除fed_funds_rate
            else:
                obs.extend([0] * 9)
        action, _ = model.predict(np.array(obs, dtype=np.float32))
        ppo_action = ["買入", "賣出", "持有"][action]
        # 最終決策：結合多框架信號和 PPO
        if buy_score &gt; 0.6 and ppo_action == "買入":
            return "買入"
        elif sell_score &gt; 0.6 and ppo_action == "賣出":
            return "賣出"
        return "持有"
    except Exception as e:
        logging.error(f"決策錯誤: {e}")
        return "持有"
def backtest(df: pd.DataFrame, strategy: callable, initial_capital: float = 10000, spread: float = 0.0002) -&gt; dict:
    """回測：模擬交易，計算績效指標，包含持倉管理。"""
    # 函數說明：模擬歷史數據上的交易決策，計算最終資本、夏普比率等指標。
    capital = initial_capital
    position_size = 0.0
    total_cost = 0.0
    entry_price = None
    trades = []
    equity_curve = []
    for i, row in df.iterrows():
        if not check_resources():
            logging.warning("資源不足，跳過交易")
            continue
        action = strategy(row)
        current_price = row['close']
        sentiment = row.get('sentiment', 0.0)
        atr = row['ATR']
        stop_loss_distance = abs(calculate_stop_loss(current_price, atr) - current_price)
        calc_position = calculate_position_size(initial_capital, 0.01, stop_loss_distance, sentiment)
        # 關鍵邏輯：處理買入/賣出決策，包括平倉和開倉。
        if action == "買入" and position_size &lt;= 0:
            if position_size &lt; 0: # 平空頭
                profit = (entry_price - current_price) * abs(position_size)
                capital += profit
                trades.append({
                    'date': row['date'],
                    'action': '平空',
                    'price': current_price,
                    'profit': profit,
                    'position_size': position_size
                })
            position_size = calc_position
            total_cost = current_price * position_size
            entry_price = current_price
            leverage_cost = abs(position_size) * 0.0001  # 假設 0.01% 融資成本
            capital -= leverage_cost
            trades.append({
                'date': row['date'],
                'action': '買入',
                'price': current_price,
                'profit': 0.0,
                'position_size': position_size
            })
        elif action == "賣出" and position_size &gt;= 0:
            if position_size &gt; 0: # 平多頭
                profit = (current_price - entry_price) * position_size
                capital += profit
                trades.append({
                    'date': row['date'],
                    'action': '平多',
                    'price': current_price,
                    'profit': profit,
                    'position_size': position_size
                })
            position_size = -calc_position
            total_cost = -current_price * abs(position_size)
            entry_price = current_price
            leverage_cost = abs(position_size) * 0.0001  # 假設 0.01% 融資成本
            capital -= leverage_cost
            trades.append({
                'date': row['date'],
                'action': '賣出',
                'price': current_price,
                'profit': 0.0,
                'position_size': position_size
            })
        equity_curve.append(capital + (current_price - entry_price) * position_size if position_size != 0 else capital)
    equity_series = pd.Series(equity_curve)
    returns = equity_series.pct_change().dropna()
    return {
        "final_capital": capital,
        "sharpe_ratio": (returns.mean() * 252 - 0.02) / (returns.std() * np.sqrt(252)) if returns.std() != 0 else 0,
        "max_drawdown": (equity_series / equity_series.cummax() - 1).min(),
        "win_rate": len([r for r in returns if r &gt; 0]) / len(returns) if len(returns) &gt; 0 else 0,
        "trades": trades
    }
def connect_ib(host='127.0.0.1', port=7497, client_id=1):
    """連接 IB API：用於實時交易。
    邏輯：建立連接，返回 IB 物件。
    """
    # 函數說明：連接 Interactive Brokers API 用於實時交易。
    ib = IB()
    ib.connect(host, port, client_id)
    return ib
def execute_trade(ib, action: str, price: float, quantity: float, stop_loss: float, take_profit: float):
    """執行交易：使用括號訂單。
    邏輯：根據行動創建訂單，附加止損/止盈。
    """
    # 函數說明：使用 BracketOrder 執行買賣訂單，並附加止損和止盈。
    contract = Forex('USDJPY')
    for attempt in range(3):
        try:
            if action == "買入":
                order = BracketOrder('BUY', quantity, price, takeProfitPrice=take_profit, stopLossPrice=stop_loss)
            elif action == "賣出":
                order = BracketOrder('SELL', quantity, price, takeProfitPrice=stop_loss, stopLossPrice=take_profit)
            else:
                return
            trade = ib.placeOrder(contract, order)
            ib.sleep(1)
            if trade.orderStatus.status in ['Filled', 'Submitted']:
                logging.info(f"訂單狀態: {trade.orderStatus.status}")
                return
            else:
                logging.warning(f"訂單失敗: {trade.orderStatus.status}, 重試 {attempt + 1}/3")
        except Exception as e:
            logging.error(f"交易執行錯誤: {e}, 重試 {attempt + 1}/3")
        ib.sleep(2 ** attempt * 2)
    logging.error("交易執行失敗，超過最大重試次數")</code></pre></li>

        <li><h3>File: .\utils.py</h3>
        <pre><code>import torch
import onnxruntime as ort
import redis
import aiohttp
import logging
import os
import sqlite3
import pandas as pd
from dotenv import load_dotenv
from cryptography.fernet import Fernet
from pathlib import Path
from datetime import datetime
import json
import traceback
import aiofiles
import asyncio
import random
# 設置日誌
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s - [%(module)s]',
    handlers=[
        logging.FileHandler('C:/Trading/logs/app.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
# 全局快取變數，用於確保配置僅載入一次
_config_cache = None

# 全局 Redis 客戶端
_redis_client = None

# 加密 API 密鑰（補充安全性）
key = b'_eIKG0YhiJCyBQ-VvxAsx8LT3Vow-k0hE-i0iwK9wwM=' # 安全儲存
cipher = Fernet(key)

def encrypt_key(api_key: str) -&gt; bytes:
    """加密 API 密鑰。"""
    return cipher.encrypt(api_key.encode())
    
def decrypt_key(encrypted: bytes) -&gt; str:
    """解密 API 密鑰。"""
    return cipher.decrypt(encrypted).decode()
    
def get_redis_client(config: dict) -&gt; redis.Redis:
    """獲取 Redis 客戶端，單例模式。"""
    global _redis_client
    if _redis_client is None and config.get('system_config', {}).get('use_redis', True):
        try:
            _redis_client = redis.Redis(host='localhost', port=6379, db=0)
            _redis_client.ping()
            logging.info("Redis 客戶端初始化成功")
        except redis.RedisError as e:
            logging.warning(f"無法初始化 Redis 客戶端: {str(e)}")
            _redis_client = None
    return _redis_client

async def fetch_api_data(url: str, headers: dict = None, params: dict = None) -&gt; dict:
    """通用 API 數據獲取函數，支援快取和重試。"""
    cache_key = f"api_{url}_{params.get('start_time', '')}_{params.get('end_time', '')}"
    redis_client = get_redis_client(load_settings())
    if redis_client:
        try:
            cached = redis_client.get(cache_key)
            if cached:
                logging.info(f"從 Redis 快取載入 API 數據: {cache_key}")
                return json.loads(cached)
        except redis.RedisError as e:
            logging.warning(f"Redis 快取查詢失敗: {str(e)}")
    
    async with aiohttp.ClientSession() as session:
        for attempt in range(5):
            try:
                async with session.get(url, headers=headers, params=params, timeout=10) as response:
                    data = await response.json()
                    if redis_client:
                        try:
                            redis_client.setex(cache_key, 3600, json.dumps(data))
                            logging.info(f"已快取 API 數據至 Redis: {cache_key}")
                        except redis.RedisError as e:
                            logging.warning(f"無法快取至 Redis: {str(e)}")
                    return data
            except Exception as e:
                if attempt == 4:
                    logging.error(f"API 獲取失敗: {str(e)}")
                    return {}
                await asyncio.sleep(2 ** attempt * 4)
    return {}
async def initialize_db(db_path: str):
    """初始化 SQLite 資料庫，創建 OHLC、indicators、economic_calendar、sentiment_data、tweets 和 trades 表格。"""
    # 從 load_settings 獲取 root_dir
    config = load_settings()
    root_dir = config.get('system_config', {}).get('root_dir', str(Path(db_path).parent))
    
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    if os.path.exists(db_path):
        try:
            conn = sqlite3.connect(db_path, timeout=10)
            cursor = conn.cursor()
            cursor.execute("SELECT 1 FROM sqlite_master")
            cursor.close()
            conn.close()
        except sqlite3.DatabaseError:
            logging.warning(f"資料庫檔案 {db_path} 損壞，將刪除並重新創建")
            os.remove(db_path)
    try:
        conn = sqlite3.connect(db_path, timeout=10)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS ohlc (
                date DATETIME,
                open REAL,
                high REAL,
                low REAL,
                close REAL,
                volume INTEGER,
                n INTEGER,
                vw REAL,
                timeframe TEXT,
                PRIMARY KEY (date, timeframe)
            )
        """)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS indicators (
                date DATETIME,
                indicator TEXT,
                value REAL,
                timeframe TEXT,
                PRIMARY KEY (date, indicator, timeframe)
            )
        """)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS economic_calendar (
                date DATETIME,
                event TEXT,
                impact TEXT,
                fed_funds_rate REAL,
                PRIMARY KEY (date, event)
            )
        """)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sentiment_data (
                date DATETIME,
                sentiment REAL,
                PRIMARY KEY (date)
            )
        """)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS tweets (
                date DATETIME,
                tweet_id TEXT,
                text TEXT,
                PRIMARY KEY (date, tweet_id)
            )
        """)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS trades (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME,
                symbol TEXT,
                price REAL,
                action TEXT,
                volume REAL,
                stop_loss REAL,
                take_profit REAL
            )
        """)
        conn.commit()
        cursor.close()
        conn.close()
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 資料庫初始化成功：{db_path}")
        await backup_database(db_path, root_dir)
        logging.info(f"Database initialized: {db_path}")
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 資料庫初始化失敗：{str(e)}")
        logging.error(f"Database initialization failed: {str(e)}, traceback={traceback.format_exc()}")
        raise

async def save_data(df: pd.DataFrame, timeframe: str, db_path: str, data_type: str = 'ohlc') -&gt; bool:
    """將數據增量儲存到 SQLite 資料庫。"""
    loop = asyncio.get_event_loop()
    try:
        def sync_save_data():
            conn = sqlite3.connect(db_path, timeout=10)
            cursor = conn.cursor()
            if data_type == 'ohlc':
                ohlc_columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'n', 'vw']
                df_to_save = df[ohlc_columns].copy()
                df_to_save['timeframe'] = timeframe
                cursor.execute("SELECT MAX(date) FROM ohlc WHERE timeframe = ?", (timeframe,))
                last_date = cursor.fetchone()[0]
                if last_date:
                    df_to_save = df_to_save[df_to_save['date'] &gt; pd.to_datetime(last_date)]
                if not df_to_save.empty:
                    logging.info(f"Inserting data: min_date={df_to_save['date'].min()}, max_date={df_to_save['date'].max()}, rows={len(df_to_save)}")
                    df_to_save.to_sql('ohlc', conn, if_exists='append', index=False)
                    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 增量儲存 {len(df_to_save)} 行 OHLC 數據至 SQLite：{timeframe}")
                    logging.info(f"Incrementally saved {len(df_to_save)} OHLC rows to SQLite: timeframe={timeframe}")
            elif data_type == 'indicators':
                ohlc_columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'n', 'vw']
                indicator_columns = [col for col in df.columns if col not in ohlc_columns + ['event', 'impact', 'sentiment', 'fed_funds_rate']]
                if indicator_columns:
                    indicators_df = df[['date'] + indicator_columns].melt(id_vars=['date'], var_name='indicator', value_name='value')
                    indicators_df['timeframe'] = timeframe
                    cursor.execute("SELECT MAX(date) FROM indicators WHERE timeframe = ?", (timeframe,))
                    last_date = cursor.fetchone()[0]
                    if last_date:
                        indicators_df = indicators_df[indicators_df['date'] &gt; pd.to_datetime(last_date)]
                    if not indicators_df.empty:
                        indicators_df.to_sql('indicators', conn, if_exists='append', index=False)
                        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 增量儲存 {len(indicators_df)} 行技術指標至 SQLite：{timeframe}")
                        logging.info(f"Incrementally saved {len(indicators_df)} indicator rows to SQLite: timeframe={timeframe}")
            elif data_type == 'economic':
                economic_columns = ['date', 'event', 'impact', 'fed_funds_rate']
                df_to_save = df[economic_columns].copy()
                cursor.execute("SELECT MAX(date) FROM economic_calendar")
                last_date = cursor.fetchone()[0]
                if last_date:
                    df_to_save = df_to_save[df_to_save['date'] &gt; pd.to_datetime(last_date)]
                if not df_to_save.empty:
                    logging.info(f"Inserting data: min_date={df_to_save['date'].min()}, max_date={df_to_save['date'].max()}, rows={len(df_to_save)}")
                    df_to_save.to_sql('economic_calendar', conn, if_exists='append', index=False)
                    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 增量儲存 {len(df_to_save)} 行經濟日曆數據至 SQLite")
                    logging.info(f"Incrementally saved {len(df_to_save)} economic calendar rows to SQLite")
            elif data_type == 'sentiment':
                sentiment_columns = ['date', 'sentiment']
                df_to_save = df[sentiment_columns].copy()
                cursor.execute("SELECT MAX(date) FROM sentiment_data")
                last_date = cursor.fetchone()[0]
                if last_date:
                    df_to_save = df_to_save[df_to_save['date'] &gt; pd.to_datetime(last_date)]
                if not df_to_save.empty:
                    logging.info(f"Inserting data: min_date={df_to_save['date'].min()}, max_date={df_to_save['date'].max()}, rows={len(df_to_save)}")
                    df_to_save.to_sql('sentiment_data', conn, if_exists='append', index=False)
                    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 增量儲存 {len(df_to_save)} 行情緒數據至 SQLite")
                    logging.info(f"Incrementally saved {len(df_to_save)} sentiment rows to SQLite")
            elif data_type == 'tweets':
                tweets_columns = ['date', 'tweet_id', 'text']
                df_to_save = df[tweets_columns].copy()
                cursor.execute("SELECT MAX(date) FROM tweets")
                last_date = cursor.fetchone()[0]
                if last_date:
                    df_to_save = df_to_save[df_to_save['date'] &gt; pd.to_datetime(last_date)]
                if not df_to_save.empty:
                    logging.info(f"Inserting data: min_date={df_to_save['date'].min()}, max_date={df_to_save['date'].max()}, rows={len(df_to_save)}")
                    df_to_save.to_sql('tweets', conn, if_exists='append', index=False)
                    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 增量儲存 {len(df_to_save)} 行推文數據至 SQLite")
                    logging.info(f"Incrementally saved {len(df_to_save)} tweet rows to SQLite")
            conn.commit()
            conn.close()
            return True
        return await loop.run_in_executor(None, sync_save_data)
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {data_type} 數據儲存失敗：{str(e)}")
        logging.error(f"Failed to save {data_type} data to SQLite: {str(e)}, traceback={traceback.format_exc()}")
        return False
async def backup_database(db_path: str, root_dir: str):
    """備份 SQLite 資料庫。"""
    backup_dir = Path(root_dir) / 'backups'
    backup_dir.mkdir(parents=True, exist_ok=True)
    backup_file = backup_dir / f"trading_data_{datetime.now().strftime('%Y%m%d')}.db"
    try:
        async with aiofiles.open(db_path, mode='rb') as src, aiofiles.open(backup_file, mode='wb') as dst:
            content = await src.read()
            await dst.write(content)
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 資料庫已備份至 {backup_file}")
        logging.info(f"Database backed up to {backup_file}")
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 資料庫備份失敗：{str(e)}")
        logging.error(f"Database backup failed: {str(e)}, traceback={traceback.format_exc()}")
async def save_periodically(df_buffer: pd.DataFrame, timeframe: str, db_path: str, root_dir: str, data_type: str = 'ohlc'):
    """定期將緩衝區數據保存到 SQLite 並進行每日備份。"""
    save_interval = 1800 if timeframe == '1 hour' else 3 * 3600
    while True:
        try:
            if not df_buffer.empty:
                await save_data(df_buffer, timeframe, db_path, data_type)
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} {data_type} 數據已增量保存至 SQLite")
                logging.info(f"Data incrementally saved to SQLite: timeframe={timeframe}, data_type={data_type}")
            if datetime.now().hour == 0 and datetime.now().minute &lt; 5:
                await backup_database(db_path, root_dir)
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 資料庫已備份")
                logging.info("Database backed up")
            await asyncio.sleep(save_interval)
        except Exception as e:
            print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 定期保存失敗：{str(e)}")
            logging.error(f"Periodic save failed: {str(e)}, traceback={traceback.format_exc()}")
def load_settings():
    """載入所有設定檔案並生成 requirements.txt。"""
    global _config_cache
    if _config_cache is not None:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 從快取載入配置")
        logging.info("Loaded config from cache")
        return _config_cache
    config = {}
    root_dir = "C:\\Trading"
    config_dir = Path(root_dir) / "config"
    config_dir.mkdir(parents=True, exist_ok=True)
    default_api_key = {}
    default_trading_params = {
        "max_position_size": 10000,
        "risk_per_trade": 0.01,
        "price_diff_threshold": {"high_volatility": 0.005, "normal": 0.003},
        "rsi_overbought": 70,
        "rsi_oversold": 30,
        "stoch_overbought": 80,
        "stoch_oversold": 20,
        "adx_threshold": 25,
        "obv_window": 14,
        "capital": 10000,
        "risk_percent": 0.01,
        "atr_threshold": 0.02,
        "min_backtest_days": 180,
        "ppo_learning_rate": 0.0003,
        "ppo_timesteps": 10000
    }
    default_system_config = {
        "data_source": "polygon",
        "symbol": "USDJPY=X",
        "timeframe": "1d",
        "root_dir": "C:\\Trading",
        "db_path": "C:\\Trading\\data\\trading_data.db",
        "proxies": {
            "http": "http://proxy1.scig.gov.hk:8080",
            "https": "http://proxy1.scig.gov.hk:8080"
        },
        "use_redis": False,  # 禁用 Redis 避免連線錯誤
        "dependencies": [],
        "model_dir": "models",
        "model_periods": ["short_term", "medium_term", "long_term"]
    }
    config_files = {
        'api_key': config_dir / 'api_key.json',
        'trading_params': config_dir / 'trading_params.json',
        'system_config': config_dir / 'system_config.json'
    }
    try:
        for key, file_path in config_files.items():
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    config[key] = json.load(f)
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 成功載入設定檔：{file_path}")
                logging.info(f"Successfully loaded config file: {file_path}")
            else:
                if key == 'api_key':
                    default = default_api_key
                elif key == 'trading_params':
                    default = default_trading_params
                elif key == 'system_config':
                    default = default_system_config
                config[key] = default
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(default, f, indent=4, ensure_ascii=False)
                print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 設定檔不存在，已創建預設：{file_path}")
                logging.info(f"Created default config file: {file_path}")
        if 'api_key' in config:
            for k, v in config['api_key'].items():
                if v:  # 僅加密非空密鑰
                    config['api_key'][k] = encrypt_key(v)
        system_config = config.get('system_config', {})
        dependencies = system_config.get('dependencies', [])
        if not dependencies:
            dependencies = ["pandas&gt;=2.0.0",
                "yfinance&gt;=0.2.0",
                "requests&gt;=2.28.0",
                "textblob&gt;=0.17.0",
                "torch&gt;=2.0.0",
                "scikit-learn&gt;=1.3.0",
                "xgboost&gt;=2.0.0",
                "lightgbm&gt;=4.0.0",
                "onnx&gt;=1.14.0",
                "onnxruntime&gt;=1.16.0",
                "transformers&gt;=4.30.0",
                "stable-baselines3&gt;=2.0.0",
                "pandas-ta&gt;=0.3.0",
                "aiosqlite&gt;=0.19.0",
                "gymnasium&gt;=0.29.0",
                "python-dotenv&gt;=1.0.0",
                "redis&gt;=5.0.0",
                "streamlit&gt;=1.25.0",
                "prometheus-client&gt;=0.17.0",
                "ib-insync&gt;=0.9.0",
                "cryptography&gt;=41.0.0",
                "scipy&gt;=1.10.0",
                "numpy&gt;=1.24.0",
                "joblib&gt;=1.3.0",
                "psutil&gt;=5.9.0",
                "onnxmltools&gt;=1.11.0",
                "onnxconverter-common&gt;=1.13.0",
                "aiohttp&gt;=3.8.0",
                "aiofiles&gt;=23.1.0",
                "investpy&gt;=1.0.0",
                "torch-directml&gt;=0.2.0"
            ]
            system_config['dependencies'] = dependencies
            with open(config_files['system_config'], 'w', encoding='utf-8') as f:
                json.dump(system_config, f, indent=4, ensure_ascii=False)
            logging.info("Filled default dependencies in system_config.json")
        requirements_path = Path(root_dir) / 'requirements.txt'
        with open(requirements_path, 'w', encoding='utf-8') as f:
            for dep in dependencies:
                f.write(f"{dep}\n")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已生成/更新 requirements.txt：{requirements_path}")
        logging.info(f"Generated requirements.txt: {requirements_path}")
        _config_cache = config
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 設定檔載入成功！")
        logging.info("Config files loaded successfully")
        return config
    except Exception as e:
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 設定檔載入失敗：{str(e)}")
        logging.error(f"Failed to load config files: {str(e)}, traceback={traceback.format_exc()}")
        return {}
def check_hardware():
    """硬體檢測：檢測 GPU/NPU/CPU 並為不同模型指定設備。"""
    try:
        import torch_directml
        gpu_available = torch_directml.is_available()
        device = torch_directml.device() if gpu_available else torch.device('cpu')
        logging.info(f"使用裝置: {device}")
    except ImportError:
        device = torch.device('cpu')
        logging.info("回退到 CPU")
    providers = ort.get_available_providers()
    onnx_provider = 'VitisAIExecutionProvider' if 'VitisAIExecutionProvider' in providers else 'CUDAExecutionProvider' if 'CUDAExecutionProvider' in providers else 'CPUExecutionProvider'
    logging.info(f"ONNX provider: {onnx_provider}")
    device_config = {
        'lstm': device,
        'finbert': device,
        'xgboost': torch.device('cpu'),
        'randomforest': torch.device('cpu'),
        'lightgbm': torch.device('cpu'),
        'timeseries_transformer': device,
        'distilbert': device,
        'ppo': device
    }
    try:
        session = ort.InferenceSession('models/lstm_model_quantized.onnx', providers=[onnx_provider])
    except Exception as e:
        logging.warning(f"Failed to load ONNX session: {str(e)}, using CPU")
        session = None
    return device_config, session
# 引入代理快取
_proxy_cache = None
def get_proxy(config: dict) -&gt; dict:
    """獲取代理設置，支援快取並檢查環境變數，確保只載入一次。"""
    global _proxy_cache
    if _proxy_cache is not None:
        logging.info("從快取載入代理設置")
        return _proxy_cache

    proxies = config.get('system_config', {}).get('proxies', {})
    if not proxies:
        # 檢查環境變數作為備用
        http_proxy = os.getenv('HTTP_PROXY')
        https_proxy = os.getenv('HTTPS_PROXY')
        if http_proxy or https_proxy:
            proxies = {'http': http_proxy, 'https': https_proxy}
            logging.info(f"從環境變數載入代理: {proxies}")
        else:
            logging.info("無代理設置")
            proxies = {}
    
    _proxy_cache = proxies
    logging.info(f"代理設置已快取: {proxies}")
    return proxies

async def test_proxy(proxy: dict) -&gt; bool:
    """測試代理是否可用，連續測試 3 次以確保穩定性。"""
    if not proxy:
        return True
    test_url = "https://www.google.com"
    success_count = 0
    for _ in range(3):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(test_url, proxy=proxy.get('http'), timeout=5) as response:
                    if response.status == 200:
                        success_count += 1
        except Exception:
            continue
        await asyncio.sleep(1)
    if success_count &gt;= 2:
        logging.info(f"代理測試成功: {proxy}，成功次數={success_count}/3")
        return True
    logging.warning(f"代理測試失敗: {proxy}，成功次數={success_count}/3")
    return False
def setup_proxy():
    """設置全局代理：從環境變數載入並設置到 os.environ，僅執行一次。"""
    if os.getenv('HTTP_PROXY') and os.getenv('HTTPS_PROXY'):
        logging.info("全局代理已設置，跳過重複設置")
        return
    load_dotenv()
    proxy = os.getenv('HTTP_PROXY')
    if proxy:
        os.environ['http_proxy'] = proxy
        os.environ['https_proxy'] = proxy
        logging.info(f"全局代理設置: {proxy}")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 全局代理設置成功")
    else:
        logging.info("無全局代理設置")
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 無全局代理設置")
def clear_proxy_cache():
    """清除代理快取。"""
    global _proxy_cache
    _proxy_cache = None
    logging.info("代理快取已清除")
def check_volatility(atr: float, threshold: float = 0.02) -&gt; bool:
    """檢查波動：若 ATR &gt; 閾值，暫停。"""
    if atr &gt; threshold:
        logging.warning("高波動偵測")
        return False
    return True
def clear_config_cache():
    """清除配置快取。"""
    global _config_cache
    _config_cache = None
    logging.info("Config cache cleared")
def filter_future_dates(df: pd.DataFrame) -&gt; pd.DataFrame:
    """過濾未來日期數據。"""
    if not df.empty and 'date' in df.columns:
        current_time = pd.to_datetime(datetime.now())
        initial_rows = len(df)
        df = df[df['date'] &lt;= current_time].copy()
        print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} 已過濾未來日期，初始行數={initial_rows}，剩餘行數={len(df)}")
        logging.info(f"Filtered future dates, initial_rows={initial_rows}, remaining_rows={len(df)}")
    return df</code></pre></li>

        <li><h3>File: .\config\system_config.json</h3>
        <pre><code>{
    "data_source": "polygon",
    "symbol": "USDJPY=X",
    "timeframe": "1d",
    "capital": 10000,
    "risk_percent": 0.01,
    "atr_threshold": 0.02,
    "root_dir": "C:\\Trading",
    "db_path": "C:\\Trading\\data\\trading_data.db",
    "min_backtest_days": 180,
    "indicators": {
        "RSI": true,
        "MACD": true,
        "ATR": true,
        "Stochastic": true,
        "Bollinger": true,
        "EMA": true
    },
  "proxies": {
        "http": "http://proxy1.scig.gov.hk:8080",
        "https": "http://proxy1.scig.gov.hk:8080"
  },
    "dependencies": ["pandas&gt;=2.0.0",
            "yfinance&gt;=0.2.0",
            "requests&gt;=2.28.0",
            "textblob&gt;=0.17.0",
            "torch&gt;=2.0.0",
            "scikit-learn&gt;=1.3.0",
            "xgboost&gt;=2.0.0",
            "lightgbm&gt;=4.0.0",
            "onnx&gt;=1.14.0",
            "onnxruntime&gt;=1.16.0",
            "transformers&gt;=4.30.0",
            "stable-baselines3&gt;=2.0.0",
            "pandas-ta&gt;=0.3.0",
            "aiosqlite&gt;=0.19.0",
            "gymnasium&gt;=0.29.0",
            "python-dotenv&gt;=1.0.0",
            "redis&gt;=5.0.0",
            "streamlit&gt;=1.25.0",
            "prometheus-client&gt;=0.17.0",
            "ib-insync&gt;=0.9.0",
            "cryptography&gt;=41.0.0",
            "scipy&gt;=1.10.0",
            "numpy&gt;=1.24.0",
            "joblib&gt;=1.3.0",
            "psutil&gt;=5.9.0",
            "onnxmltools&gt;=1.11.0",
            "onnxconverter-common&gt;=1.13.0",
            "aiohttp&gt;=3.8.0",
            "aiofiles&gt;=23.1.0",
            "investpy&gt;=1.0.0",
            "torch-directml&gt;=0.2.0"
],
    "model_dir": "models",
    "model_periods": ["short_term", "medium_term", "long_term"],
    "use_redis": false
}</code></pre></li>

        <li><h3>File: .\config\trading_params.json</h3>
        <pre><code>{
    "ppo_learning_rate": 0.0003,
    "ppo_timesteps": 10000,
"max_position_size": 10000,
    "risk_per_trade": 0.01,
    "price_diff_threshold": {"high_volatility": 0.005, "normal": 0.003},
    "rsi_overbought": 70,
    "rsi_oversold": 30,
    "stoch_overbought": 80,
    "stoch_oversold": 20,
    "adx_threshold": 25,
    "obv_window": 14,
    "capital": 10000,
    "risk_percent": 0.01,
    "atr_threshold": 0.02,
    "min_backtest_days": 180
}</code></pre></li>

    </ul>
    <h2>Markdown Files</h2>
    <ul>
        <li><h3>Markdown File: .\USD_JPY_Trading_Software_Guide.md</h3>
        <pre><code class="markdown"># USD/JPY 自動投資軟件 Python 設計指南

## 摘要
- **目標**：開發針對 USD/JPY 外匯交易的自動投資軟件，利用 AI 模型進行價格預測、情緒分析、交易策略制定和風險管理，整合免費 API 提供實時與歷史數據。軟件支援回測、模擬交易和實時交易，強調模塊化設計以便擴展。
- **硬體**：Aoostar GT37（AMD Ryzen AI 9 HX 370，12 核 24 線程 CPU、Radeon 890M GPU、XDNA 2 NPU）。
- **市場特性**：USD/JPY 高波動性，對宏觀經濟數據（利率、通脹、GDP）敏感，需結合新聞/社群媒體情緒和技術指標分析。
- **焦點**：AI 模型選擇、API 整合、硬體優化、軟件架構、程式碼範例、測試指南和模型更新策略。

## 1. 軟件架構
軟件採用模塊化設計，遵循 MVC（Model-View-Controller）模式變體：資料模塊（Model）、AI 處理模塊（Controller）和報告模塊（View）。主要資料流：API → 資料預處理 → AI 模型 → 交易決策 → 日誌/報告。

### 主要模塊
- `data_acquisition.py`：處理 API 呼叫、資料獲取和預處理。
- `ai_models.py`：定義和訓練 AI 模型（價格預測、情緒分析等）。
- `trading_strategy.py`：整合模型輸出制定交易決策，使用 PPO 等。
- `risk_management.py`：計算止損/止盈、倉位大小，監控波動性。
- `main.py`：入口點，載入配置、硬體檢測、執行回測/實時模式。
- `utils.py`：公用函數，如錯誤處理、日誌、資料庫操作。
- `config/`：存放 JSON 配置檔。
- `reports/`：輸出報告檔（回測/實時）。
- `models/`：儲存訓練好的模型檔（ONNX 格式）。
- `tests/`：存放單元測試和回測測試。

### 資料流範例
1. 從 API 獲取資料 → 儲存到資料庫。
2. 預處理（計算指標、填補缺失值） → 輸入 AI 模型。
3. 模型輸出（預測、情緒分數） → 交易策略決策。
4. 執行模擬交易或實時交易 → 生成報告。

### 檔案結構
```
usd_jpy_trader/
├── main.py
├── data_acquisition.py
├── ai_models.py
├── trading_strategy.py
├── risk_management.py
│   test_data_acquisition.py
│   test_backtest.py
├── utils.py
├── config/
│   ├── system_config.json
│   └── api_key.json
├── models/
│   ├── lstm_model.onnx
│   ├── TimeSeriesTransformer.onnx
│   ├── FinBERT.onnx
│   ├── DistilBERT.onnx
│   ├── PPO.onnx
│   ├── XGBoost.onnx
│   └── LightGBM.onnx
├── reports/
│   ├── backtest_report_[日期]_[時間].csv
│   └── live_report_[日期]_[時間].csv
├── logs/
│   ├── backtest_log_[日期]_[時間].csv
│   └── live_log_[日期]_[時間].log
├── data/
│   └── trades.db
└── requirements.txt
```

## 2. AI 模型選擇
以下模型組合適用於 USD/JPY 的價格預測、情緒分析、交易策略和風險管理，優化於 HX 370 硬體。

| 功能         | 模型                     | 適用性                                       | HX 370 優化                              |
|--------------|--------------------------|----------------------------------------------|------------------------------------------|
| 價格預測     | LSTM/TimeSeriesTransformer | 捕捉時間序列模式，預測短期/中期走勢         | GPU（DirectML）加速矩陣運算；NPU（ONNX）實時推理 |
| 情緒分析     | FinBERT/DistilBERT       | 分析新聞/X 貼文，判斷央行政策影響           | CPU 批量文本處理；GPU 加速 BERT 推理     |
| 交易策略     | PPO（強化學習）          | 動態調整買賣決策，優化利潤/Sharpe Ratio    | GPU 加速訓練；NPU 低延遲決策            |
| 風險管理     | XGBoost/LightGBM         | 預測波動性，生成止損/止盈建議               | CPU 多線程訓練；GPU（LightGBM）加速      |

### 輸入資料
- **歷史價格**：OHLC（開盤、最高、最低、收盤）。
- **技術指標**：RSI、EMA、MACD、布林帶、ATR、ADX、Ichimoku Cloud、Stochastic Oscillator。
- **經濟數據**：聯邦基金利率、日本央行政策、GDP、通脹。
- **情緒來源**：新聞文章、X 貼文、央行聲明。

### 框架
- PyTorch/TensorFlow，搭配 Hugging Face Transformers。
- Stable-Baselines3（PPO），Pandas_TA/TA-Lib（技術指標）。
- ONNX 格式支援 DirectML 和 Vitis AI 優化。

### 模型訓練流程
- **數據分割**：80% 訓練集，10% 驗證集，10% 測試集。
- **損失函數**：MSE（價格預測）、CrossEntropy（情緒分析）。
- **超參數範圍**：
  - LSTM：層數（2-4）、隱藏單元（50-200）、學習率（0.0001-0.001）。
  - PPO：學習率（0.0003）、總步數（10,000-100,000）。

### 範例程式碼（ai_models.py）
```python
import torch
from torch import nn
from transformers import pipeline
from sklearn.model_selection import train_test_split
from torch.optim import Adam

class LSTMModel(nn.Module):
    def __init__(self, input_size=10, hidden_size=50, num_layers=2, output_size=1):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        return self.fc(h_n[-1])

def train_lstm_model(df: pd.DataFrame, epochs: int = 50):
    X = df[['Close', 'RSI', 'MACD']].values
    y = df['Close'].shift(-1).dropna().values
    X_train, X_test, y_train, y_test = train_test_split(X[:-1], y, test_size=0.2)
    model = LSTMModel()
    optimizer = Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(torch.tensor(X_train, dtype=torch.float32))
        loss = criterion(output, torch.tensor(y_train, dtype=torch.float32))
        loss.backward()
        optimizer.step()
    torch.save(model.state_dict(), 'models/lstm_model.pth')
    return model

# 情緒分析範例
sentiment_pipeline = pipeline('sentiment-analysis', model='finbert')
def predict_sentiment(text):
    return sentiment_pipeline(text)[0]['label']  # 返回 'positive', 'negative' 或 'neutral'

# 情緒分數整合
def integrate_sentiment(sentiment: str) -&gt; float:
    if sentiment == 'positive':
        return 0.1  # 增加買入傾向
    elif sentiment == 'negative':
        return -0.1  # 增加賣出傾向
    return 0.0
```

## 3. API 選擇與應用
以下免費 API 提供 USD/JPY 數據，適用於 Python 3.10 環境。

| API          | 描述                                       | 適用性                                       | HX 370 優化                       | 限制                        | 設置範例                                                                 |
|--------------|--------------------------------------------|----------------------------------------------|-----------------------------------|-----------------------------|--------------------------------------------------------------------------|
| **yfinance** | 免費獲取 USD/JPY OHLC 數據，支持多時間框架 | 價格預測（OHLC 輸入 LSTM）；技術指標（搭配 Pandas_TA） | CPU 處理數據；GPU 加速指標計算 | 數據可能有延遲，偶有缺失   | `df = yf.download("USDJPY=X", start="2025-01-01", end="2025-08-25")`    |
| **fixer.io** | 170+ 貨幣歷史匯率，每小時更新（歐洲央行） | 回測（日收盤數據輸入 Transformer）           | CPU 處理回應；GPU 清理數據       | 免費 100 請求/月，無 OHLC  | `url = "https://data.fixer.io/api/timeseries?access_key=KEY&amp;symbols=USDJPY"` |
| **Polygon.io** | 外匯 OHLC 數據，免費計劃有限             | 價格預測（OHLC 輸入模型）；交易策略（實時數據） | CPU/GPU 處理大規模數據          | 免費 5 次/分鐘             | `url = "https://api.polygon.io/v2/aggs/ticker/C:USDJPY/range/1/day/2023-08-23/2025-08-22?adjusted=true&amp;sort=asc&amp;apiKey=KEY"` |
| **FRED API** | 美國經濟數據（利率、GDP 等）              | 情緒分析（數據輸入 FinBERT）；風險管理       | CPU 批量處理；GPU 加速分析       | 更新頻率低（日/月）        | `url = "https://api.stlouisfed.org/fred/series/observations?series_id=FEDFUNDS&amp;api_key=KEY"` |
| **X API**    | X 貼文數據，用於情緒分析                  | 情緒分析（貼文輸入 FinBERT）                | CPU 處理文本；GPU 加速推理       | 免費 10,000 次/月，僅公開貼文 | `url = "https://api.x.com/2/tweets/search/recent?query=USDJPY"`        |
| **investpy** | 經濟事件日曆，支援日期/國家篩選           | 情緒分析（事件輸入 FinBERT）；風險管理       | CPU 處理事件；GPU 加速分析       | 依網站速率，需遵守條款     | `investpy.get_economic_calendar(from_date="2025-01-01")`               |
| **FCS API**  | 外匯歷史/即時匯率，免費計劃有限          | 價格預測（歷史數據）；交易策略（即時匯率）   | CPU 處理數據；GPU 清理           | 免費 500 次/月，無技術指標 | `url = "https://fcsapi.com/api-v3/forex/history?symbol=USD/JPY&amp;access_key=KEY"` |

### 備註
- **技術指標計算**：使用 Pandas_TA 計算技術指標：
  ```python
  import pandas_ta as ta
  import pandas as pd

  def compute_indicators(df: pd.DataFrame) -&gt; pd.DataFrame:
      df['RSI'] = ta.rsi(df['Close'], length=14)
      df['MACD'] = ta.macd(df['Close'], fast=12, slow=26, signal=9)['MACD_12_26_9']
      df['ATR'] = ta.atr(df['High'], df['Low'], df['Close'], length=14)
      df['BB_upper'], df['BB_middle'], df['BB_lower'] = ta.bbands(df['Close'], length=20, std=2).iloc[:, :3]
      return df
  ```
- **實時交易**：建議使用 Interactive Brokers（IB）API（需帳戶），回測使用 yfinance 或 fixer.io。
- **資料預處理**：處理缺失值，使用前向填補：
  ```python
  df = df.fillna(method='ffill').fillna(method='bfill')
  ```
- **API 限額管理**：使用 Redis 快取減少 API 呼叫：
  ```python
  import redis
  import json
  from datetime import timedelta

  redis_client = redis.Redis(host='localhost', port=6379, db=0)

  def cache_data(key: str, data: dict, expiry: int = 3600):
      redis_client.setex(key, timedelta(seconds=expiry), json.dumps(data))

  def get_cached_data(key: str) -&gt; dict:
      data = redis_client.get(key)
      return json.loads(data) if data else None
  ```

## 4. 技術指標應用
| 指標                  | 類型       | 應用                                                 | HX 370 優化                      |
|-----------------------|------------|------------------------------------------------------|----------------------------------|
| EMA/SMA               | 趨勢       | 價格預測（LSTM 輸入）；交易策略（交叉訊號）         | CPU 多線程；GPU 批量計算        |
| MACD                  | 趨勢       | 價格預測（Transformer 輸入）；風險管理（背離預測）   | GPU 加速矩陣運算                |
| Ichimoku Cloud        | 趨勢       | 交易策略（雲層突破輸入 PPO）                        | NPU 實時計算                    |
| RSI                   | 動量       | 價格預測（輔助情緒）；風險管理（極值止損）          | CPU 批量計算                    |
| Stochastic Oscillator | 動量       | 交易策略（%K/%D 交叉輸入 PPO）                     | GPU 加速多時間框                |
| Bollinger Bands       | 動量       | 風險管理（突破輸入 XGBoost）                        | NPU 實時監測                    |
| ATR                   | 波動性     | 風險管理（動態止損）；交易策略（高 ATR 避免入場）   | CPU 處理歷史數據                |
| ADX                   | 趨勢強度   | 交易策略（確認 PPO 趨勢）；價格預測（濾噪音）       | GPU 加速過濾                    |

## 5. 基本邏輯應用
### 配置
從 `config/system_config.json` 和 `api_key.json` 載入設置，使用環境變數儲存敏感資訊：
```python
import json
import os
from dotenv import load_dotenv

load_dotenv()
with open('config/system_config.json') as f:
    config = json.load(f)
api_key = os.getenv('POLYGON_API_KEY')  # 從環境變數讀取
```

### 代理
檢查是否需要 PROXY，動態提取代理資訊：
```python
import os

def setup_proxy():
    proxy = os.getenv('HTTP_PROXY')
    if proxy:
        os.environ['http_proxy'] = proxy
        os.environ['https_proxy'] = proxy
        print("代理已設置:", proxy)
    else:
        print("無代理設置")
```

### 模型更新策略
- **性能檢查**：使用滾動窗口（最近 1年數據）計算預測誤差，若 RMSE 超過閾值（0.01），觸發重新訓練。
- **數據分佈變化檢測**：使用 KS 檢驗比較新舊數據分佈。
  ```python
  from scipy.stats import ks_2samp
  import os

  def detect_drift(old_data: pd.DataFrame, new_data: pd.DataFrame, threshold: float = 0.05) -&gt; bool:
      stat, p_value = ks_2samp(old_data['Close'], new_data['Close'])
      return p_value &lt; threshold

  def update_model(df: pd.DataFrame, model_path: str = 'models/lstm_model.pth'):
      if not os.path.exists(model_path) or detect_drift(df.iloc[:-1000], df.iloc[-1000:]):
          model = train_lstm_model(df)
          torch.save(model.state_dict(), model_path)
          logging.info("模型已更新並保存至 %s", model_path)
      else:
          model = LSTMModel()
          model.load_state_dict(torch.load(model_path))
          logging.info("載入現有模型: %s", model_path)
      return model
  ```

### 資料庫
使用 `aiosqlite` 管理策略數據，定義表結構：
```sql
CREATE TABLE trades (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME,
    symbol TEXT,
    price REAL,
    action TEXT,
    volume REAL,
    stop_loss REAL,
    take_profit REAL
);
```
範例程式碼：
```python
import aiosqlite
import pandas as pd
import asyncio

async def save_data(df: pd.DataFrame, db_path: str = "data/trades.db"):
    async with aiosqlite.connect(db_path) as db:
        await db.execute("""
            CREATE TABLE IF NOT EXISTS trades (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME,
                symbol TEXT,
                price REAL,
                action TEXT,
                volume REAL,
                stop_loss REAL,
                take_profit REAL
            )
        """)
        await df.to_sql("trades", db, if_exists="append", index=False)

async def load_data(db_path: str = "data/trades.db") -&gt; pd.DataFrame:
    async with aiosqlite.connect(db_path) as db:
        return pd.read_sql("SELECT * FROM trades", db)
```

### 日誌
- 回測：`reports/backtest_log_[日期]_[時間].log`
- 實時：`reports/live_log_[日期]_[時間].log`
- 使用 logging 模塊記錄錯誤和決策：
  ```python
  import logging
  import datetime

  logging.basicConfig(
      filename=f'logs/app_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
      level=logging.INFO,
      format='%(asctime)s - %(levelname)s - %(message)s'
  )
  logging.info("交易決策: 買入 USD/JPY")
  ```

### 交易報告
- 回測：`reports/backtest_report_[日期]_[時間].csv`
- 實時：`reports/live_report_[日期]_[時間].csv`
  ```

### py to txt小工具
- 制作一個獨立程式將本軟件的所有PY FILEs 內容合拼到一個TXT文件方便與GROK交流

### 錯誤處理
- **API 失敗**：延時 5 秒用備用 API 重試，價格數據優先順序：Polygon.io → yfinance → FCS API；情緒數據：X API → investpy。
  ```python
  import time
  import requests
  import logging

  def fetch_data(primary_url, backup_urls):
      try:
          response = requests.get(primary_url)
          response.raise_for_status()
          return response.json()
      except Exception as e:
          logging.error(f"Primary API 失敗: {e}")
          time.sleep(5)
          for backup in backup_urls:
              try:
                  response = requests.get(backup)
                  response.raise_for_status()
                  return response.json()
              except Exception as e:
                  logging.error(f"Backup API {backup} 失敗: {e}")
                  continue
          raise ValueError("所有 API 失敗")
  ```

### 邊緣案例處理
檢測高波動性事件（如黑天鵝），暫停交易：
```python
def check_volatility(atr: float, threshold: float = 0.02) -&gt; bool:
    if atr &gt; threshold:
        logging.warning("高波動事件檢測，暫停交易")
        return False
    return True
```

## 6. 交易執行邏輯
### 決策流程
1. 獲取最新資料和技術指標。
2. 運行價格預測和情緒分析。
3. 使用 PPO 產生行動（買、賣、持有）。
4. 應用風險管理：計算 ATR-based 止損/止盈，確定倉位大小。
5. 執行模擬交易或透過 IB API 執行，設訂單時間上限，混合使用市價單及限價單。

### Interactive Brokers API 整合
- **訂單類型**：
  - **限價單**：低波動期確保價格控制。
  - **止損單**：動態調整基於 ATR。
  - **括號訂單**：自動附加止損/止盈。
  - **OCA 群組**：多訂單互斥，避免過度暴露。
- **策略**：高波動期使用 OCA 結合止損單；低波動期使用限價單等待突破；最大暴露限額為資本的 5%。
- **範例程式碼（trading_strategy.py，需 pip install ib_insync）**：
  ```python
  from ib_insync import IB, Forex, LimitOrder, BracketOrder
  import logging

  def connect_ib(host='127.0.0.1', port=7497, client_id=1):
      ib = IB()
      ib.connect(host, port, client_id)
      return ib

  def execute_trade(ib, action: str, price: float, quantity: float, stop_loss: float, take_profit: float):
      contract = Forex('USDJPY')
      if action == "買入":
          order = BracketOrder('BUY', quantity, price, takeProfitPrice=take_profit, stopLossPrice=stop_loss)
      elif action == "賣出":
          order = BracketOrder('SELL', quantity, price, takeProfitPrice=stop_loss, stopLossPrice=take_profit)
      else:
          return
      trade = ib.placeOrder(contract, order)
      ib.sleep(1)  # 等待確認
      logging.info(f"訂單狀態: {trade.orderStatus.status}")
  ```

### 交易策略範例（trading_strategy.py）
```python
from stable_baselines3 import PPO
import gym
import numpy as np

class ForexEnv(gym.Env):
    def __init__(self, df, spread=0.0002):
        super().__init__()
        self.df = df
        self.spread = spread
        self.current_step = 0
        self.action_space = gym.spaces.Discrete(3)  # 買、賣、持有
        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float32)
    
    def reset(self):
        self.current_step = 0
        return self._get_obs()
    
    def _get_obs(self):
        return self.df[['Close', 'RSI', 'MACD']].iloc[self.current_step].values
    
    def step(self, action):
        price = self.df['Close'].iloc[self.current_step]
        reward = 0
        if action == 0:  # 買入
            reward -= self.spread
        elif action == 1:  # 賣出
            reward -= self.spread
        self.current_step += 1
        done = self.current_step &gt;= len(self.df)
        return self._get_obs(), reward, done, {}

def train_ppo(env):
    model = PPO("MlpPolicy", env, verbose=1, learning_rate=0.0003)
    model.learn(total_timesteps=10000)
    model.save("models/ppo_model")
    return model

def make_decision(model, obs):
    action, _ = model.predict(obs)
    if action == 0:
        return "買入"
    elif action == 1:
        return "賣出"
    else:
        return "持有"
```

### 風險管理
- **止損/止盈**：
  ```python
  def calculate_stop_loss(current_price: float, atr: float, multiplier: float = 2) -&gt; float:
      return current_price - (multiplier * atr)  # 適用於長倉

  def calculate_take_profit(current_price: float, atr: float, multiplier: float = 2) -&gt; float:
      return current_price + (multiplier * atr)  # 適用於長倉
  ```
- **倉位大小**：
  ```python
  def calculate_position_size(capital: float, risk_percent: float, stop_loss_distance: float) -&gt; float:
      return (capital * risk_percent) / stop_loss_distance
  ```

## 7. 硬體優化
- **環境**：Python 3.10，依賴清單：
  ```bash
  pip install torch-directml onnxruntime transformers stable-baselines3 pandas-ta lightgbm aiosqlite requests gym pytest python-dotenv redis streamlit prometheus-client ib_insync cryptography scipy yfinance pandas numpy torch sklearn
  ```
- **requirements.txt**：
  ```
  torch-directml
  onnxruntime
  transformers
  stable-baselines3
  pandas-ta
  lightgbm
  aiosqlite
  requests
  gym
  pytest
  python-dotenv
  redis
  streamlit
  prometheus-client
  ib_insync
  cryptography
  scipy
  yfinance
  pandas
  numpy
  torch
  sklearn
  ```
- **GPU（Radeon 890M）**：DirectML 加速模型訓練/推理。
- **NPU（XDNA 2）**：ONNX 模型量化至 INT8，實現低延遲決策。
- **CPU**：24 線程處理批量數據。
- **記憶體**：32GB LPDDR5X-8000MHz 支援大型模型。

### 硬體檢測
```python
import torch
import onnxruntime as ort
import logging

def check_hardware():
    # 檢測 GPU (DirectML)
    try:
        import torch_directml
        if torch_directml.is_available():
            gpu_device = torch_directml.device()
            logging.info("GPU (DirectML) 支援，使用 Radeon 890M 加速。")
        else:
            gpu_device = torch.device('cpu')
            logging.info("GPU (DirectML) 不支援，回退到 CPU。")
    except ImportError:
        gpu_device = torch.device('cpu')
        logging.info("torch_directml 未安裝，回退到 CPU。")
    # 檢測 NPU (XDNA 2 with ONNX)
    providers = ort.get_available_providers()
    if 'VitisAIExecutionProvider' in providers:
        logging.info("NPU (XDNA 2) 支援，使用 ONNX INT8 量化低延遲決策。")
        session = ort.InferenceSession('model.onnx', providers=['VitisAIExecutionProvider'])
    else:
        logging.info("NPU (XDNA 2) 不支援，回退到 CPU。")
        session = ort.InferenceSession('model.onnx', providers=['CPUExecutionProvider'])
    return gpu_device, session
```

### 多線程優化
```python
from concurrent.futures import ThreadPoolExecutor

def batch_process_data(data_list):
    with ThreadPoolExecutor(max_workers=24) as executor:
        results = list(executor.map(compute_indicators, data_list))
    return results
```

## 8. 回測框架
- **回測邏輯**：模擬資金管理（初始資本、交易成本、滑點），計算績效指標（Sharpe Ratio、最大回撤、勝率、年化回報）。
- **範例程式碼**：
  ```python
  import pandas as pd
  import numpy as np

  def calculate_sharpe_ratio(returns: pd.Series, risk_free_rate: float = 0.02) -&gt; float:
      return (returns.mean() * 252 - risk_free_rate) / (returns.std() * np.sqrt(252))

  def calculate_max_drawdown(equity_curve: pd.Series) -&gt; float:
      return (equity_curve / equity_curve.cummax() - 1).min()

  def backtest(df: pd.DataFrame, strategy: callable, initial_capital: float = 10000, spread: float = 0.0002) -&gt; dict:
      capital = initial_capital
      positions = []
      equity_curve = []
      for i, row in df.iterrows():
          action = strategy(row)
          if action == "買入":
              positions.append(row['Close'] + spread)
          elif action == "賣出" and positions:
              capital += (row['Close'] - positions.pop()) * 1000
          equity_curve.append(capital)
      returns = pd.Series(equity_curve).pct_change().dropna()
      return {
          "final_capital": capital,
          "sharpe_ratio": calculate_sharpe_ratio(returns),
          "max_drawdown": calculate_max_drawdown(pd.Series(equity_curve)),
          "win_rate": len([r for r in returns if r &gt; 0]) / len(returns) if returns else 0
      }
  ```

## 9. 測試與部署
### 單元測試
使用 pytest 進行單元測試。
- **範例（tests/test_data_acquisition.py）**：
  ```python
  import pytest
  from data_acquisition import fetch_data

  def test_fetch_data():
      primary_url = "https://example.com"
      backup_urls = ["https://backup1.com", "https://backup2.com"]
      data = fetch_data(primary_url, backup_urls)
      assert isinstance(data, dict)
  ```
- **回測測試**：驗證 Sharpe Ratio &gt; 0。
  ```python
  import pytest
  from trading_strategy import backtest

  def test_backtest():
      df = pd.DataFrame({'Close': [100, 101, 102], 'RSI': [50, 60, 70], 'MACD': [0, 0.1, 0.2]})
      result = backtest(df, lambda x: "買入" if x['RSI'] &gt; 60 else "持有")
      assert result['sharpe_ratio'] &gt; 0
  ```
- **黑天鵝測試**：
  ```python
  def test_high_volatility():
      df_high_vol = pd.DataFrame({'Close': [100, 120, 80], 'ATR': [0.03, 0.04, 0.05]})
      assert not check_volatility(df_high_vol['ATR'].mean())
  ```

### 部署
根目錄:"C:\Trading"

api_key.json:
{
  "fmp_api_key": "uUFGG1dA6jFHeFgf1XHNJpt4obL3uJfS",
  "fred_api_key": "81ffe6474ddf0bc54bb72e0f26918fcc",
  "x_bearer_token": "AAAAAAAAAAAAAAAAAAAAAIKJ3gEAAAAA0zJ2qC00iUuIIxFK70aMIN1zk5s%3D913jMzVxOA8azfWxs2unudDsAXF6OKjZDlIUn43pxrp94oZnps",
  "FCS_API_Key": "bN1WbcGvs5Bn9KjXxWkKwQPVoNsf1HJQ5",  
  "currencylayer_API_Key": "a013222877c20f54227a3f436c5e6031",
  "fixer_API_Key": "def5c1c73ccc267cf9edc754f61d3c0b",
  "exchangeratesio_API_Key": "4ed1a1e247b8c6cb4231f038deef375d",
  "exchangeratehost_API_Key": "cc83b43b2847ee6482d25b4273c0f32f",
  "marketstack_API_Key": "163ccf72c25454f66775951018ad6b8f",
  "twelvedata_API_Key": "583a65d8b04b47bda1cddec0c5a83a51",
  "polygon_api_key": "VWSmFPevufl8GGsAZC9VqbiW0xco0ZpO"
}
system_config.json:
{
    "data_source": "yfinance",
    "symbol": "USDJPY=X",
    "timeframe": "1d",
    "capital": 10000,
    "risk_percent": 0.01,
    "atr_threshold": 0.02,
    "root_dir": "C:\\Trading",
    "db_path": "C:\\Trading\\data\\trades.db",
    "min_backtest_days": 180,
  "proxies": {
        "http": "http://proxy1.scig.gov.hk:8080",
        "https": "http://proxy1.scig.gov.hk:8080"
  },
    "dependencies": [],
    "model_dir": "models",
    "model_periods": ["short_term", "medium_term", "long_term"]
}

使用 Docker 容器化：
```dockerfile
FROM python:3.10
WORKDIR /app
COPY . /app
RUN pip install -r requirements.txt
CMD ["python", "main.py"]
```

### 用戶介面
使用 Streamlit 提供 Web 儀表板：
```python
import streamlit as st
import pandas as pd

def display_dashboard(df: pd.DataFrame):
    st.title("USD/JPY 交易儀表板")
    st.line_chart(df['Close'])
    st.write("最新決策:", df['Action'].iloc[-1])
    st.write("最新技術指標:", df[['RSI', 'MACD', 'ATR']].iloc[-1])
    override_action = st.selectbox("手動覆寫決策", ["無", "買入", "賣出", "持有"])
    if override_action != "無":
        logging.info(f"用戶覆寫: {override_action}")
```

## 10. 限制與注意事項
- **API 限制**：免費 API 請求限額低（100-10,000 次/月），不適合高頻交易。
- **數據質量**：免費數據可能有延遲或缺失，需預處理。
- **法律合規**：
  - 遵守 API 條款和外匯監管（如 FCA、CFTC），避免過度爬蟲。
  - 合規檢查：
    ```python
    def compliance_check(trade: dict) -&gt; bool:
        return trade['leverage'] &lt;= 30  # 假設監管要求槓桿不超過 30:1
    ```
- **安全性**：
  - 使用 python-dotenv 儲存 API 密鑰：
    ```python
    from dotenv import load_dotenv
    import os

    load_dotenv()
    api_key = os.getenv('POLYGON_API_KEY')
    ```
  - 加密敏感數據：
    ```python
    from cryptography.fernet import Fernet

    key = Fernet.generate_key()  # 儲存於安全處
    cipher = Fernet(key)
    encrypted_key = cipher.encrypt(api_key.encode())
    # 解密：decrypted = cipher.decrypt(encrypted_key).decode()
    ```
- **建議**：
  - 回測驗證模型，確保 Sharpe Ratio &gt; 1.0，最大回撤 &lt; 20%。
  - 結合 OANDA 或 IB API 執行實時交易。
  - 每月檢查模型，使用滾動窗口訓練。
  - 使用 Prometheus 和 Grafana 監控性能：
    ```yaml
    global:
      scrape_interval: 15s
    scrape_configs:
      - job_name: 'usd_jpy_trader'
        static_configs:
          - targets: ['localhost:8000']
    ```
- **風險披露**：本軟件僅供教育用途，不構成投資建議。用戶需自行承擔風險。

## 11. 主程式範例（main.py）
```python
import asyncio
import logging
import pandas as pd
import streamlit as st
from data_acquisition import fetch_data, compute_indicators
from ai_models import LSTMModel, predict_sentiment, train_lstm_model, integrate_sentiment
from trading_strategy import train_ppo, make_decision, ForexEnv, connect_ib, execute_trade
from risk_management import calculate_stop_loss, calculate_position_size
from utils import check_hardware, update_model, check_volatility
from display import display_dashboard

async def main(mode: str = 'backtest'):
    logging.basicConfig(filename='logs/app.log', level=logging.INFO)
    gpu_device, onnx_session = check_hardware()
    # 獲取資料
    df = pd.DataFrame()  # 假設從 API 或資料庫獲取
    df = compute_indicators(df)
    await save_data(df)
    # 檢查波動性
    if not check_volatility(df['ATR'].mean()):
        return
    # 價格預測
    model = update_model(df, 'models/lstm_model.pth').to(gpu_device)
    input_data = df[['Close', 'RSI', 'MACD']].values[-10:]
    prediction = model(torch.tensor(input_data, dtype=torch.float32))
    # 情緒分析
    sentiment = predict_sentiment("USD/JPY expected to rise due to Fed policy")
    sentiment_score = integrate_sentiment(sentiment)
    # 交易決策
    env = ForexEnv(df)
    ppo_model = train_ppo(env)
    action = make_decision(ppo_model, input_data)
    # 風險管理
    atr = df['ATR'].iloc[-1]
    stop_loss = calculate_stop_loss(df['Close'].iloc[-1], atr)
    take_profit = calculate_take_profit(df['Close'].iloc[-1], atr)
    position_size = calculate_position_size(capital=10000, risk_percent=0.01, stop_loss_distance=df['Close'].iloc[-1] - stop_loss)
    logging.info(f"行動: {action}, 預測: {prediction}, 情緒: {sentiment}, 止損: {stop_loss}, 止盈: {take_profit}, 倉位大小: {position_size}")
    # 實時執行
    if mode == 'live':
        ib = connect_ib()
        execute_trade(ib, action, df['Close'].iloc[-1], position_size, stop_loss, take_profit)
    # 可視化
    if mode == 'live':
        display_dashboard(df)

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="USD/JPY 自動交易系統")
    parser.add_argument('--mode', choices=['backtest', 'live'], default='backtest', help="運行模式")
    args = parser.parse_args()
    asyncio.run(main(args.mode))
```

## 12. 用戶指南
### 安裝步驟
1. 安裝 Python 3.10。
2. 運行 `pip install -r requirements.txt`。
3. 設定 .env 檔案（e.g., `POLYGON_API_KEY=your_key`）。
4. 運行 `python main.py --mode=backtest`。

### 運行模式
- **回測**：`--mode=backtest`（模擬歷史數據）。
- **實時**：`--mode=live`（需 IB 帳戶，啟動 Streamlit 儀表板）。

### 貢獻指南
- Fork 儲存庫，提交 PR。
- 測試覆蓋率 &gt; 80%。</code></pre></li>

    </ul>
</body>
</html>